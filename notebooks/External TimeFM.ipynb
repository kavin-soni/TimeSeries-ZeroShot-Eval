{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZrQtq5UnGSq"
      },
      "outputs": [],
      "source": []
    },
    {
      "metadata": {
        "id": "Kd7SkUt1nvSg"
      },
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# CELL 1: SETUP & IMPORTS\n",
        "# ==========================================\n",
        "import timesfm  # pip install git+https://github.com/google-research/timesfm.git\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Generic Hardware Check\n",
        "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Using device: {device.upper()}\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 2: LOAD MODEL\n",
        "# ==========================================\n",
        "# Using the 500M model from Hugging Face\n",
        "CHECKPOINT_REPO = \"google/timesfm-2.0-500m-pytorch\"\n",
        "\n",
        "print(f\"Loading TimesFM from {CHECKPOINT_REPO}...\")\n",
        "\n",
        "tfm = timesfm.TimesFm(\n",
        "    context_len=2048,\n",
        "    horizon_len=14,         # M4 Daily Horizon\n",
        "    input_patch_len=32,\n",
        "    output_patch_len=128,\n",
        "    num_layers=50,\n",
        "    model_dims=1280,\n",
        "    backend=device\n",
        ")\n",
        "\n",
        "tfm.load_from_checkpoint(repo_id=CHECKPOINT_REPO)\n",
        "print(\"✅ Model loaded.\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 3: LOAD DATA\n",
        "# ==========================================\n",
        "# Ensure you have 'm4_daily.csv' in your data folder\n",
        "# Columns required: 'unique_id', 'ds', 'y'\n",
        "DATA_PATH = \"../data/m4_daily.csv\"\n",
        "\n",
        "print(\"Loading M4 Data...\")\n",
        "try:\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    print(f\"✅ Loaded {len(df)} rows. Unique Series: {df['unique_id'].nunique()}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: {DATA_PATH} not found.\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 4: ZERO-SHOT INFERENCE\n",
        "# ==========================================\n",
        "print(\"Running Forecast (Freq='D')...\")\n",
        "\n",
        "forecast_df = tfm.forecast_on_df(\n",
        "    inputs=df,\n",
        "    freq=\"D\",\n",
        "    value_name=\"y\",\n",
        "    num_jobs=-1\n",
        ")\n",
        "print(\"✅ Inference Complete.\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 5: ROBUST METRICS (Correct MASE)\n",
        "# ==========================================\n",
        "def calculate_metrics_robust(y_true, y_pred, y_train_hist, seasonality=1):\n",
        "    \"\"\"\n",
        "    Calculates metrics with correct MASE denominator (Training History).\n",
        "    \"\"\"\n",
        "    # Standard Errors\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # sMAPE\n",
        "    epsilon = 1e-10\n",
        "    smape = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred) + epsilon))\n",
        "\n",
        "    # Correct MASE Denominator\n",
        "    if len(y_train_hist) > seasonality:\n",
        "        naive_mae = np.mean(np.abs(y_train_hist[seasonality:] - y_train_hist[:-seasonality]))\n",
        "    elif len(y_train_hist) > 1:\n",
        "        naive_mae = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        naive_mae = epsilon\n",
        "\n",
        "    mase = mae / (naive_mae + epsilon)\n",
        "\n",
        "    return {\"MAE\": mae, \"RMSE\": rmse, \"sMAPE\": smape, \"MASE\": mase}\n",
        "\n",
        "# --- Evaluation Loop ---\n",
        "print(\"Calculating Metrics...\")\n",
        "metrics_list = []\n",
        "\n",
        "# Group by Series (Assuming 'df' contains history + test truth)\n",
        "for uid, group in df.groupby('unique_id'):\n",
        "    pred_row = forecast_df[forecast_df['unique_id'] == uid]\n",
        "    if pred_row.empty: continue\n",
        "\n",
        "    y_hist = group['y'].values\n",
        "    # Hold out last 14 steps for truth\n",
        "    y_true = y_hist[-14:]\n",
        "    y_hist_train = y_hist[:-14]\n",
        "    y_pred = pred_row['timesfm'].values[:14]\n",
        "\n",
        "    if len(y_true) == len(y_pred):\n",
        "        m = calculate_metrics_robust(y_true, y_pred, y_hist_train, seasonality=1)\n",
        "        m['unique_id'] = uid\n",
        "        metrics_list.append(m)\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"FINAL M4 RESULTS (n={len(metrics_df)})\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Mean MASE:  {metrics_df['MASE'].mean():.4f}\")\n",
        "print(f\"Mean sMAPE: {metrics_df['sMAPE'].mean():.4f}%\")\n",
        "print(\"=\"*40)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "RJuruAthn4fv"
      },
      "cell_type": "code",
      "source": [
        "# [Setup & Imports same as above]\n",
        "# ...\n",
        "\n",
        "# ==========================================\n",
        "# CELL 2: LOAD MODEL\n",
        "# ==========================================\n",
        "tfm = timesfm.TimesFm(\n",
        "    context_len=2048,\n",
        "    horizon_len=96,         # Traffic Horizon\n",
        "    input_patch_len=32,\n",
        "    output_patch_len=128,\n",
        "    num_layers=50,\n",
        "    model_dims=1280,\n",
        "    backend=device\n",
        ")\n",
        "tfm.load_from_checkpoint(repo_id=\"google/timesfm-2.0-500m-pytorch\")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 3: DATA & INFERENCE\n",
        "# ==========================================\n",
        "DATA_PATH = \"../data/traffic.csv\"\n",
        "# ... Load Data Code ...\n",
        "\n",
        "print(\"Running Forecast (Freq='H')...\")\n",
        "# Note: Hourly frequency\n",
        "forecast_df = tfm.forecast_on_df(\n",
        "    inputs=df,\n",
        "    freq=\"H\",\n",
        "    value_name=\"y\",\n",
        "    num_jobs=-1\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# CELL 5: METRICS (Seasonality=24)\n",
        "# ==========================================\n",
        "# ... calculate_metrics_robust function definition ...\n",
        "\n",
        "# Loop change:\n",
        "    # y_true = y_hist[-96:]\n",
        "    # y_hist_train = y_hist[:-96]\n",
        "    # m = calculate_metrics_robust(y_true, y_pred, y_hist_train, seasonality=24) # 24 for Hourly Traffic\n",
        "\n",
        "# ... Summary Print ..."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "slnznR2gp_iY"
      },
      "cell_type": "code",
      "source": [
        "# Same structure as Traffic.\n",
        "# Change DATA_PATH to \"../data/etth1.csv\"\n",
        "# Keep Freq=\"H\"\n",
        "# Keep Seasonality=24 for MASE calculation."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "94LNVrfXqChy"
      },
      "cell_type": "code",
      "source": [
        "# Same structure as M4.\n",
        "# Change DATA_PATH to \"../data/exchange.csv\"\n",
        "# Change Horizon to 96\n",
        "# Keep Freq=\"D\"\n",
        "# Keep Seasonality=1 for MASE calculation."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "r3_N9ye2qFGz"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "UEFxzkbBqMAK"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, full_df):\n",
        "\n",
        "    print(\"\\n--- Training XGBoost Model (Legacy API) ---\")\n",
        "\n",
        "    if len(X_train) == 0:\n",
        "        print(\"ERROR: Training set is empty after cleaning.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'rmse',\n",
        "        'eta': 0.1,\n",
        "        'max_depth': 8,\n",
        "        'seed': 42,\n",
        "        'nthread': -1,\n",
        "        'tree_method': 'hist'\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    eval_list = [(dval, 'validation')]\n",
        "\n",
        "    start_time = time.time()\n",
        "    bst = xgb.train(\n",
        "        xgb_params, dtrain, num_boost_round=1000,\n",
        "        evals=eval_list, early_stopping_rounds=50, verbose_eval=False\n",
        "    )\n",
        "    print(f\"Training complete in {time.time() - start_time:.2f} seconds. ({bst.best_iteration} rounds)\")\n",
        "\n",
        "    # Prediction\n",
        "    y_pred_test = bst.predict(dtest, iteration_range=(0, bst.best_iteration))\n",
        "\n",
        "    # --- AGGREGATION (OPTIMIZED) ---\n",
        "    print(\"\\n--- Calculating Final Benchmarks (Optimized) ---\")\n",
        "\n",
        "    # 1. Prepare Results DataFrame\n",
        "    results_df = full_df.loc[X_test.index, ['series_id', 'sales']].copy()\n",
        "    results_df['y_pred'] = y_pred_test\n",
        "\n",
        "    # 2. Pre-Calculate History for MASE Denominator (THE FIX)\n",
        "    print(\"Pre-grouping historical data for fast lookup...\")\n",
        "\n",
        "    # Identify indices that are NOT in the test set (i.e., Train + Validation)\n",
        "    history_indices = full_df.index.difference(X_test.index)\n",
        "    history_df = full_df.loc[history_indices]\n",
        "\n",
        "    # Create a GroupBy object - this creates the index map once!\n",
        "    history_groups = history_df.groupby('series_id')['sales']\n",
        "\n",
        "    metrics_list = []\n",
        "    unique_ids = results_df['series_id'].unique()\n",
        "\n",
        "    print(f\"Aggregating metrics for {len(unique_ids)} unique series...\")\n",
        "\n",
        "    # 3. Fast Loop\n",
        "    # Use groupby on results_df to iterate efficiently\n",
        "    for series_id, series_res in results_df.groupby('series_id'):\n",
        "\n",
        "        # Instant lookup from the pre-grouped history object\n",
        "        if series_id in history_groups.groups:\n",
        "            history_series = history_groups.get_group(series_id).values\n",
        "        else:\n",
        "            history_series = np.array([])\n",
        "\n",
        "        metrics = calculate_metrics(\n",
        "            series_res['sales'].values,\n",
        "            series_res['y_pred'].values,\n",
        "            history_series\n",
        "        )\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"\\nEvaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (Main execution remains the same) ...\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test, full_df = load_and_prepare_m4_data()\n",
        "    final_metrics_xgb = train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, full_df)\n",
        "    print(\"\\n✅ M4 XGBoost Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3KJnsam0qQOZ"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Replace with your actual file paths\n",
        "INPUT_TRAIN_FILE = 'M4_Daily_Train_PLX.csv'\n",
        "INPUT_TEST_FILE = 'M4_Daily_Test_PLX.csv'\n",
        "FORECAST_HORIZON = 14\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"\n",
        "    Calculates standard M4 metrics.\n",
        "    Crucially, for M4 Daily data, the MASE denominator is the\n",
        "    Mean Absolute Error of the Naive 1 (Lag 1) forecast.\n",
        "    \"\"\"\n",
        "    y_pred = np.maximum(0, y_pred) # Clip negatives if strictly positive domain\n",
        "\n",
        "    # 1. Standard Errors\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 2. sMAPE (Symmetric Mean Absolute Percentage Error)\n",
        "    # Formula: 200 * mean(|y - y_hat| / (|y| + |y_hat|))\n",
        "    denom = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denom != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 3. MASE (Mean Absolute Scaled Error)\n",
        "    # Numerator: MAE of the model\n",
        "    # Denominator: MAE of Naive 1 Forecast on Training History (Lag 1)\n",
        "    if len(y_train_hist) > 1:\n",
        "        # np.diff calculates y[t] - y[t-1], which is the Naive 1 error\n",
        "        naive_mae = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        naive_mae = 0.0\n",
        "\n",
        "    # Avoid division by zero\n",
        "    mase = mae / naive_mae if naive_mae > 1e-9 else np.nan\n",
        "\n",
        "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase}\n",
        "\n",
        "def run_m4_naive1_baseline():\n",
        "    print(\"Loading M4 Data\")\n",
        "    try:\n",
        "        train_df = pd.read_csv(INPUT_TRAIN_FILE)\n",
        "        test_df = pd.read_csv(INPUT_TEST_FILE)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Files not found. Please upload 'M4_Daily_Train_PLX.csv' and 'M4_Daily_Test_PLX.csv'\")\n",
        "        return\n",
        "\n",
        "    # Normalize column names\n",
        "    if 'value' in train_df.columns: train_df.rename(columns={'value': 'sales'}, inplace=True)\n",
        "    if 'value' in test_df.columns: test_df.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Pre-group for speed\n",
        "    print(f\"Analyzing {train_df['series_id'].nunique()} series...\")\n",
        "    train_groups = train_df.groupby('series_id')['sales']\n",
        "    test_groups = test_df.groupby('series_id')['sales']\n",
        "\n",
        "    metrics_list = []\n",
        "    unique_ids = test_df['series_id'].unique()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, series_id in enumerate(unique_ids):\n",
        "        # Get history and ground truth\n",
        "        y_train = train_groups.get_group(series_id).values\n",
        "        y_test = test_groups.get_group(series_id).values\n",
        "\n",
        "        # --- NAIVE 1 FORECAST (for M4 Daily) ---\n",
        "        # Logic: \"Tomorrow will be the same as today\"\n",
        "        # We take the very last value of the training set and repeat it for the horizon.\n",
        "        if len(y_train) > 0:\n",
        "            last_value = y_train[-1]\n",
        "            y_pred = np.full(FORECAST_HORIZON, last_value)\n",
        "        else:\n",
        "            y_pred = np.zeros(FORECAST_HORIZON)\n",
        "\n",
        "        # Calculate metrics\n",
        "        m = calculate_metrics(y_test, y_pred, y_train)\n",
        "        m['unique_id'] = series_id\n",
        "        metrics_list.append(m)\n",
        "\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(f\"Processed {i+1} series...\")\n",
        "\n",
        "    # Aggregate\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"FINAL M4 BASELINE RESULTS (NAIVE 1)\")\n",
        "    print(\"=\"*40)\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Total Time: {time.time() - start_time:.2f} seconds\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run_m4_naive1_baseline()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "gliCJHIpqSjW"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_TRAIN_FILE = 'M4_Daily_Train_PLX.csv'\n",
        "INPUT_TEST_FILE = 'M4_Daily_Test_PLX.csv'\n",
        "LOOKBACK_WINDOW = 30\n",
        "FORECAST_HORIZON = 14\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5  # Increased slightly as normalized data converges faster\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    # Ensure no negatives for metrics that can't handle them (like sMAPE)\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    denom = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denom != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # M4 Daily MASE denominator: Naive 1 (Lag 1)\n",
        "    if len(y_train_hist) > 1:\n",
        "        naive_mae = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        naive_mae = 0.0\n",
        "\n",
        "    mase = mae / naive_mae if naive_mae > 1e-9 else np.nan\n",
        "\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "# --- DATA PREPARATION ---\n",
        "def create_sequences(df, target_col, window, horizon):\n",
        "    \"\"\"Converts data into 3D arrays [Samples, Time Steps, 1] using the NORMALIZED column.\"\"\"\n",
        "    X, Y = [], []\n",
        "\n",
        "    # Group by series_id\n",
        "    for series_id, group in df.groupby('series_id'):\n",
        "        data = group[target_col].values\n",
        "\n",
        "        if len(data) < window + horizon:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(data) - window - horizon + 1):\n",
        "            X.append(data[i : i + window])\n",
        "            Y.append(data[i + window : i + window + horizon])\n",
        "\n",
        "    X = np.array(X).reshape(-1, window, 1)\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_and_prepare_m4_lstm():\n",
        "    print(\"Loading M4 Data...\")\n",
        "    train_df = pd.read_csv(INPUT_TRAIN_FILE)\n",
        "    test_df = pd.read_csv(INPUT_TEST_FILE)\n",
        "\n",
        "    if 'value' in train_df.columns: train_df.rename(columns={'value': 'sales'}, inplace=True)\n",
        "    if 'value' in test_df.columns: test_df.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # --- NEW: NORMALIZATION STEP ---\n",
        "    print(\"Normalizing Data per Series...\")\n",
        "    # Calculate Mean and Std for every series\n",
        "    stats = train_df.groupby('series_id')['sales'].agg(['mean', 'std']).reset_index()\n",
        "\n",
        "    # Handle constant series (std=0) to avoid division by zero\n",
        "    stats['std'] = stats['std'].replace(0, 1.0)\n",
        "\n",
        "    # Merge stats back to training data\n",
        "    train_df = train_df.merge(stats, on='series_id', how='left')\n",
        "\n",
        "    # Create 'norm_sales' column: (x - mean) / std\n",
        "    train_df['norm_sales'] = (train_df['sales'] - train_df['mean']) / train_df['std']\n",
        "\n",
        "    # Convert stats to a dictionary for easy lookup later during Inverse Transform\n",
        "    stats_dict = stats.set_index('series_id').to_dict('index')\n",
        "\n",
        "    print(\"Generating sequences from Normalized Data...\")\n",
        "    # Note: We pass 'norm_sales' as the target column\n",
        "    X_all, Y_all = create_sequences(train_df, 'norm_sales', LOOKBACK_WINDOW, FORECAST_HORIZON)\n",
        "\n",
        "    # Split sequences into Train/Val (90/10)\n",
        "    split_idx = int(len(X_all) * 0.9)\n",
        "    X_train, X_val = X_all[:split_idx], X_all[split_idx:]\n",
        "    Y_train, Y_val = Y_all[:split_idx], Y_all[split_idx:]\n",
        "\n",
        "    print(f\"Sequence Shapes -> Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "\n",
        "    # Create Prediction Input (Last window of NORMALIZED history)\n",
        "    print(\"Preparing Forecast Inputs...\")\n",
        "    X_pred_list = []\n",
        "    valid_series_ids = []\n",
        "\n",
        "    train_df_sorted = train_df.sort_values(['series_id', 'date'])\n",
        "\n",
        "    for series_id, group in train_df_sorted.groupby('series_id'):\n",
        "        hist = group['norm_sales'].values  # Grab normalized history\n",
        "\n",
        "        if len(hist) >= LOOKBACK_WINDOW:\n",
        "            X_pred_list.append(hist[-LOOKBACK_WINDOW:])\n",
        "            valid_series_ids.append(series_id)\n",
        "        else:\n",
        "            padded = np.pad(hist, (LOOKBACK_WINDOW - len(hist), 0), 'constant')\n",
        "            X_pred_list.append(padded)\n",
        "            valid_series_ids.append(series_id)\n",
        "\n",
        "    X_predict = np.array(X_pred_list).reshape(-1, LOOKBACK_WINDOW, 1)\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, X_predict, valid_series_ids, train_df, test_df, stats_dict\n",
        "\n",
        "# --- MODEL & EXECUTION ---\n",
        "def run_lstm_benchmark(X_train, Y_train, X_val, Y_val, X_predict, valid_series_ids, train_df, test_df, stats_dict):\n",
        "\n",
        "    # 1. Build Model\n",
        "    model = Sequential([\n",
        "        LSTM(units=64, return_sequences=False, activation='tanh', input_shape=(LOOKBACK_WINDOW, 1)),\n",
        "        Dense(FORECAST_HORIZON)\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    print(\"\\n--- Training LSTM Model ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, Y_val),\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 2. Predict (Output is Normalized)\n",
        "    print(\"Generating Forecasts...\")\n",
        "    Y_pred_norm = model.predict(X_predict)\n",
        "\n",
        "    # 3. Metrics & Inverse Transform\n",
        "    print(\"Calculating Metrics...\")\n",
        "    metrics_list = []\n",
        "\n",
        "    pred_map_norm = dict(zip(valid_series_ids, Y_pred_norm))\n",
        "    history_groups = train_df.groupby('series_id')['sales'] # Raw sales for Naive calc\n",
        "\n",
        "    for series_id, group in test_df.groupby('series_id'):\n",
        "        if series_id not in pred_map_norm:\n",
        "            continue\n",
        "\n",
        "        # Get Raw Truth\n",
        "        y_true = group.sort_values('date')['sales'].values\n",
        "\n",
        "        # Get Normalized Prediction\n",
        "        y_pred_n = pred_map_norm[series_id]\n",
        "\n",
        "        # --- NEW: INVERSE TRANSFORM ---\n",
        "        # Actual = Norm * Std + Mean\n",
        "        series_stats = stats_dict.get(series_id)\n",
        "        if series_stats:\n",
        "            y_pred = (y_pred_n * series_stats['std']) + series_stats['mean']\n",
        "        else:\n",
        "            y_pred = y_pred_n # Fallback (should not happen)\n",
        "\n",
        "        # Handle length mismatch\n",
        "        min_len = min(len(y_true), len(y_pred))\n",
        "        y_true = y_true[:min_len]\n",
        "        y_pred = y_pred[:min_len]\n",
        "\n",
        "        if len(y_true) == 0: continue\n",
        "\n",
        "        # Get Raw History for MASE\n",
        "        if series_id in history_groups.groups:\n",
        "            y_hist = history_groups.get_group(series_id).values\n",
        "        else:\n",
        "            y_hist = np.array([])\n",
        "\n",
        "        m = calculate_metrics(y_true, y_pred, y_hist)\n",
        "        m['unique_id'] = series_id\n",
        "        metrics_list.append(m)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*35)\n",
        "    print(\"FINAL M4 DAILY BENCHMARK RESULTS (LSTM)\")\n",
        "    print(\"=\"*35)\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Total Time: {end_time - start_time:.2f} seconds\")\n",
        "    print(\"=\"*35)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    X_train, Y_train, X_val, Y_val, X_predict, valid_series_ids, train_df, test_df, stats_dict = load_and_prepare_m4_lstm()\n",
        "    run_lstm_benchmark(X_train, Y_train, X_val, Y_val, X_predict, valid_series_ids, train_df, test_df, stats_dict)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kzySWucbqYMm"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Inw6mKFxqd0s"
      },
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#   XGBOOST FORECASTING MODEL FOR HOURLY TRAFFIC DATA\n",
        "# ==============================================================================\n",
        "# This script trains an XGBoost model on the 'Traffic_Hourly_Train.csv' dataset.\n",
        "# It uses a sliding window approach with lags (1h, 24h, 168h) to predict traffic.\n",
        "#\n",
        "# Metrics calculated: MAE, MSE, RMSE, sMAPE, MASE, Bias\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Traffic_Daily_Train.csv'\n",
        "TEST_FILE = 'Traffic_Daily_Test.csv'\n",
        "\n",
        "# Forecast Horizon: 168 hours (1 Week)\n",
        "FORECAST_HORIZON = 168\n",
        "\n",
        "# Validation size: Last 4 days (96 hours) of training data\n",
        "VALIDATION_HOURS = 96\n",
        "\n",
        "# Lag Shift: 1 means we use previous hour's data to predict current hour.\n",
        "LAG_SHIFT = 1\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # Ensure non-negative predictions (traffic cannot be negative)\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # Calculate seasonal naive MAE (Traffic is hourly, dominant seasonality is Daily = 24)\n",
        "    seasonality = 24\n",
        "    if len(y_train_hist) > seasonality:\n",
        "        naive_errors = np.abs(y_train_hist[seasonality:] - y_train_hist[:-seasonality])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Forecast Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "def generate_features(df):\n",
        "    \"\"\"Generates time-series features for Hourly Traffic data.\"\"\"\n",
        "\n",
        "    # --- 1. TIME FEATURES ---\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['weekday'] = df['date'].dt.weekday\n",
        "    df['hour'] = df['date'].dt.hour # Critical for Hourly Data\n",
        "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n",
        "\n",
        "    # --- 2. CATEGORICAL FEATURES ---\n",
        "    # Encode Series ID (Sensor Name)\n",
        "    encoder = LabelEncoder()\n",
        "    # Ensure input is uniform string to prevent TypeError\n",
        "    df['series_id_encoded'] = encoder.fit_transform(df['series_id'].astype(str))\n",
        "\n",
        "    # --- 3. LAG AND ROLLING WINDOW FEATURES ---\n",
        "    # We define a helper to shift strictly within a Series ID\n",
        "    def grouped_shift(col, shift_n):\n",
        "        return df.groupby('series_id')[col].shift(shift_n)\n",
        "\n",
        "    # Lags (Based on Hourly Logic)\n",
        "    df['lag_1']   = grouped_shift('sales', LAG_SHIFT)\n",
        "    df['lag_24']  = grouped_shift('sales', LAG_SHIFT + 23)  # 1 day ago\n",
        "    df['lag_168'] = grouped_shift('sales', LAG_SHIFT + 167) # 1 week ago\n",
        "\n",
        "    # Rolling Statistics\n",
        "    # Rolling Mean of last 24 hours (shifted to avoid leakage)\n",
        "    df['rolling_mean_24'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(24).mean()\n",
        "    )\n",
        "    df['rolling_std_24'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(24).std()\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_traffic_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found. Run the process_traffic_data.py script first.\")\n",
        "        return None\n",
        "\n",
        "    # Load with low_memory=False to suppress mixed type warnings initially\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # --- CRITICAL FIX: Force series_id to string ---\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Rename value -> sales for consistency with internal logic\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Mark splits before merging\n",
        "    df_train['is_test'] = False\n",
        "    df_test['is_test'] = True\n",
        "\n",
        "    # Combine for Feature Engineering (Preserve history at boundaries)\n",
        "    full_df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "    full_df = full_df.sort_values(['series_id', 'date'])\n",
        "\n",
        "    print(\"2. Generating Features...\")\n",
        "    full_df = generate_features(full_df)\n",
        "\n",
        "    # --- SPLITTING ---\n",
        "    # 1. Recover Test Set\n",
        "    X_test_full = full_df[full_df['is_test'] == True].copy()\n",
        "\n",
        "    # 2. Recover Train Set (All non-test data)\n",
        "    train_full = full_df[full_df['is_test'] == False].copy()\n",
        "\n",
        "    # 3. Create Validation Set from the END of the Train Set\n",
        "    max_train_date = train_full['date'].max()\n",
        "    val_start_date = max_train_date - pd.Timedelta(hours=VALIDATION_HOURS - 1)\n",
        "\n",
        "    # Split Train/Val\n",
        "    X_train_df = train_full[train_full['date'] < val_start_date]\n",
        "    X_val_df = train_full[train_full['date'] >= val_start_date]\n",
        "\n",
        "    # Define Feature Columns (Drop non-feature cols)\n",
        "    drop_cols = ['sales', 'date', 'series_id', 'is_test', 'split_type']\n",
        "    features = [c for c in full_df.columns if c not in drop_cols]\n",
        "\n",
        "    print(f\"   Features used: {features}\")\n",
        "\n",
        "    # Create Final Arrays\n",
        "    X_train = X_train_df[features]\n",
        "    y_train = X_train_df['sales']\n",
        "\n",
        "    X_val = X_val_df[features]\n",
        "    y_val = X_val_df['sales']\n",
        "\n",
        "    X_test = X_test_full[features]\n",
        "    y_test = X_test_full['sales']\n",
        "\n",
        "    # Clean NaNs caused by lags (mostly in early train data)\n",
        "    valid_indices = X_train.dropna().index\n",
        "    X_train = X_train.loc[valid_indices]\n",
        "    y_train = y_train.loc[valid_indices]\n",
        "\n",
        "    print(\"\\n--- Final Data Shapes ---\")\n",
        "    print(f\"Train Set: {len(X_train)} rows\")\n",
        "    print(f\"Val Set:   {len(X_val)} rows\")\n",
        "    print(f\"Test Set:  {len(X_test)} rows\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full\n",
        "\n",
        "def train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full_history):\n",
        "\n",
        "    print(\"\\n--- Training XGBoost Model ---\")\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'rmse',\n",
        "        'eta': 0.05,\n",
        "        'max_depth': 8,\n",
        "        'seed': 42,\n",
        "        'nthread': -1,\n",
        "        'tree_method': 'hist'\n",
        "    }\n",
        "    num_round = 1000\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    eval_list = [(dtrain, 'train'), (dval, 'validation')]\n",
        "\n",
        "    bst = xgb.train(\n",
        "        xgb_params,\n",
        "        dtrain,\n",
        "        num_round,\n",
        "        evals=eval_list,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    print(f\"   Best Iteration: {bst.best_iteration}\")\n",
        "\n",
        "    # Prediction\n",
        "    y_pred_test = bst.predict(dtest, iteration_range=(0, bst.best_iteration))\n",
        "\n",
        "    # --- METRICS CALCULATION ---\n",
        "    print(\"\\n--- Calculating Final Benchmarks ---\")\n",
        "\n",
        "    results_df = X_test_full.copy()\n",
        "    results_df['y_pred'] = y_pred_test\n",
        "    # Ensure we use the actual sales column for truth, keeping index alignment\n",
        "    results_df['sales'] = y_test\n",
        "\n",
        "    metrics_list = []\n",
        "\n",
        "    # Iterate over unique series to calculate MASE per series\n",
        "    unique_series = results_df['series_id'].unique()\n",
        "\n",
        "    for i, series_id in enumerate(unique_series):\n",
        "        if i % 100 == 0: print(f\"   Evaluated {i}/{len(unique_series)} series...\", end='\\r')\n",
        "\n",
        "        series_results = results_df[results_df['series_id'] == series_id]\n",
        "\n",
        "        # Use full training history (Train + Val) for MASE denominator\n",
        "        history_series = train_full_history[train_full_history['series_id'] == series_id]['sales'].values\n",
        "\n",
        "        metrics = calculate_metrics(\n",
        "            series_results['sales'].values,\n",
        "            series_results['y_pred'].values,\n",
        "            history_series\n",
        "        )\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"\\nEvaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load data\n",
        "    data_tuple = load_and_prepare_traffic_data()\n",
        "\n",
        "    if data_tuple:\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full = data_tuple\n",
        "\n",
        "        # Train and Eval\n",
        "        train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full)\n",
        "\n",
        "        print(\"\\n✅ Traffic XGBoost Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GGn2yjsEqgzL"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Traffic_Daily_Train.csv'\n",
        "TEST_FILE = 'Traffic_Daily_Test.csv'\n",
        "\n",
        "# PREDICTION SEASONALITY:\n",
        "# Traffic is hourly. The strongest naive predictor is usually \"same time last week\"\n",
        "# (captures weekend vs weekday differences).\n",
        "SEASONALITY = 168 # 1 Week\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # NOTE: We use 24 (Daily) for the denominator to match the XGBoost script's scaling,\n",
        "    # ensuring MASE is comparable across models.\n",
        "    mase_seasonality = 24\n",
        "    if len(y_train_hist) > mase_seasonality:\n",
        "        naive_errors = np.abs(y_train_hist[mase_seasonality:] - y_train_hist[:-mase_seasonality])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "\n",
        "def load_traffic_data():\n",
        "    \"\"\"Loads the pre-split Traffic CSVs.\"\"\"\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(f\"Error: {TRAIN_FILE} or {TEST_FILE} not found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "    print(f\"   Train Rows: {len(df_train)}\")\n",
        "    print(f\"   Test Rows:  {len(df_test)}\")\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def run_snaive_benchmark(df_train, df_test):\n",
        "    \"\"\"\n",
        "    Runs Seasonal Naive: Forecast = Last 168 hours (1 Week) of training, repeated.\n",
        "    \"\"\"\n",
        "    metrics_list = []\n",
        "\n",
        "    train_groups = df_train.groupby('series_id')\n",
        "    test_groups = df_test.groupby('series_id')\n",
        "\n",
        "    unique_ids = df_test['series_id'].unique()\n",
        "\n",
        "    print(f\"\\n2. Running Seasonal Naive Benchmark (S={SEASONALITY}) on {len(unique_ids)} series...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, series_id in enumerate(unique_ids):\n",
        "        if i > 0 and i % 100 == 0:\n",
        "            print(f\"   Processed {i}/{len(unique_ids)}...\", end='\\r')\n",
        "\n",
        "        try:\n",
        "            train_series = train_groups.get_group(series_id).sort_values('date')\n",
        "            y_train = train_series['sales'].values\n",
        "\n",
        "            test_series = test_groups.get_group(series_id).sort_values('date')\n",
        "            y_test = test_series['sales'].values\n",
        "\n",
        "            # Prediction Horizon\n",
        "            horizon = len(y_test)\n",
        "\n",
        "            # Forecast Logic: Repeat the last SEASONALITY window\n",
        "            if len(y_train) < SEASONALITY:\n",
        "                # Fallback if history is too short (rare for Traffic)\n",
        "                y_pred = np.tile(y_train[-1:], horizon)\n",
        "            else:\n",
        "                # Grab last 168 hours\n",
        "                last_season = y_train[-SEASONALITY:]\n",
        "                repetitions = int(np.ceil(horizon / SEASONALITY))\n",
        "                y_pred = np.tile(last_season, repetitions)[:horizon]\n",
        "\n",
        "            metrics = calculate_metrics(y_test, y_pred, y_train)\n",
        "            metrics['unique_id'] = series_id\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"\\n\\n--- Final SNaive Results ({total_time:.2f}s) ---\")\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_train, df_test = load_traffic_data()\n",
        "\n",
        "    if df_train is not None:\n",
        "        run_snaive_benchmark(df_train, df_test)\n",
        "        print(\"\\n✅ Traffic SNaive Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JM8kpIg9-NPa"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Traffic_Daily_Train.csv'\n",
        "TEST_FILE = 'Traffic_Daily_Test.csv'\n",
        "\n",
        "# Standard Hourly Benchmark Settings\n",
        "LOOKBACK_WINDOW = 168   # 1 Week Context\n",
        "FORECAST_HORIZON = 168  # 1 Week Prediction\n",
        "VALIDATION_STEPS = 96   # 4 Days Validation Targets\n",
        "\n",
        "# Generator Stride: Samples data every N hours to control epoch size\n",
        "# Stride 1 = Use all data (Slowest/Most Data)\n",
        "# Stride 24 = Use one sample per day per sensor (Fast/Good for benchmarks)\n",
        "STRIDE = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, mae_naive):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # Uses pre-calculated naive MAE (Daily Seasonality = 24 hours) passed as argument\n",
        "    if mae_naive > 1e-9:\n",
        "        mase = mae_forecast / mae_naive\n",
        "    else:\n",
        "        mase = np.nan\n",
        "\n",
        "    # 6. Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "# --- MEMORY-SAFE DATA GENERATOR ---\n",
        "\n",
        "class TrafficGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, lookback, horizon, batch_size=256, stride=1, shuffle=True):\n",
        "        super().__init__() # REQUIRED for Keras 3+ to avoid warnings\n",
        "        self.lookback = lookback\n",
        "        self.horizon = horizon\n",
        "        self.batch_size = batch_size\n",
        "        self.stride = stride\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Pre-process data into a dictionary of numpy arrays {series_id: array}\n",
        "        # This keeps memory minimal (just raw data, no sequence duplication)\n",
        "        self.data_dict = {}\n",
        "        self.indices = []\n",
        "\n",
        "        print(\"   Initializing Generator... (Log-Normalizing Data in Memory)\")\n",
        "\n",
        "        # Group by series and prepare data\n",
        "        for series_id, group in df.groupby('series_id'):\n",
        "            # Clean and Log Normalize ONCE here\n",
        "            raw_vals = group['sales'].values.astype('float32')\n",
        "            raw_vals = np.nan_to_num(raw_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            log_vals = np.log1p(np.maximum(0, raw_vals))\n",
        "\n",
        "            self.data_dict[series_id] = log_vals\n",
        "\n",
        "            # Calculate valid start indices\n",
        "            # A valid start index i means we can take data[i : i+lookback] as X\n",
        "            # and data[i+lookback : i+lookback+horizon] as Y\n",
        "            usable_len = len(log_vals) - lookback - horizon + 1\n",
        "            if usable_len > 0:\n",
        "                for i in range(0, usable_len, stride):\n",
        "                    self.indices.append((series_id, i))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "        print(f\"   Generator Ready: {len(self.indices)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches per epoch\n",
        "        return int(np.floor(len(self.indices) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate one batch of data\n",
        "        batch_indices = self.indices[index*self.batch_size : (index+1)*self.batch_size]\n",
        "\n",
        "        X = []\n",
        "        Y = []\n",
        "\n",
        "        for series_id, start_idx in batch_indices:\n",
        "            series_data = self.data_dict[series_id]\n",
        "\n",
        "            # X: Input Sequence\n",
        "            X.append(series_data[start_idx : start_idx + self.lookback])\n",
        "\n",
        "            # Y: Target Sequence\n",
        "            Y.append(series_data[start_idx + self.lookback : start_idx + self.lookback + self.horizon])\n",
        "\n",
        "        return np.array(X)[..., np.newaxis], np.array(Y)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "# --- LOADING ---\n",
        "\n",
        "def load_and_prepare_traffic_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found.\")\n",
        "        return None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, usecols=['series_id', 'date', 'value'], low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, usecols=['series_id', 'date', 'value'], low_memory=False)\n",
        "\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'], errors='coerce')\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'], errors='coerce')\n",
        "\n",
        "    df_train['sales'] = pd.to_numeric(df_train['sales'], errors='coerce')\n",
        "    df_test['sales'] = pd.to_numeric(df_test['sales'], errors='coerce')\n",
        "\n",
        "    df_train.dropna(subset=['date', 'sales'], inplace=True)\n",
        "    df_test.dropna(subset=['date', 'sales'], inplace=True)\n",
        "\n",
        "    df_train = df_train.sort_values(['series_id', 'date'])\n",
        "    df_test = df_test.sort_values(['series_id', 'date'])\n",
        "\n",
        "    # --- PREPARE PREDICTION INPUTS ---\n",
        "    print(\"2. Preparing Prediction Inputs (X_predict)...\")\n",
        "\n",
        "    X_predict_list = []\n",
        "    ids_list = []\n",
        "    test_ids = df_test['series_id'].unique()\n",
        "\n",
        "    # OPTIMIZATION: Group by series_id ONCE before the loop to make lookups O(1)\n",
        "    # This prevents the slow repeated filtering of the large dataframe\n",
        "    train_groups = df_train.groupby('series_id')\n",
        "\n",
        "    for series_id in test_ids:\n",
        "        # Fast retrieval using the groupby object\n",
        "        if series_id not in train_groups.groups:\n",
        "            continue\n",
        "\n",
        "        series_hist = train_groups.get_group(series_id)['sales'].values.astype('float32')\n",
        "        series_hist = np.nan_to_num(series_hist, nan=0.0)\n",
        "        series_hist = np.log1p(np.maximum(0, series_hist))\n",
        "\n",
        "        if len(series_hist) >= LOOKBACK_WINDOW:\n",
        "            X_predict_list.append(series_hist[-LOOKBACK_WINDOW:])\n",
        "            ids_list.append(series_id)\n",
        "        else:\n",
        "            padded = np.pad(series_hist, (LOOKBACK_WINDOW - len(series_hist), 0), 'constant')\n",
        "            X_predict_list.append(padded)\n",
        "            ids_list.append(series_id)\n",
        "\n",
        "    X_predict = np.array(X_predict_list, dtype='float32').reshape(-1, LOOKBACK_WINDOW, 1)\n",
        "\n",
        "    return df_train, df_test, X_predict, ids_list\n",
        "\n",
        "def build_and_run_lstm(df_train, df_test, X_predict, ids_list):\n",
        "\n",
        "    # 1. Pre-Calculate Naive MAE (MASE Denominator) to save RAM later\n",
        "    print(\"3. Pre-calculating Naive Errors (MASE Denominator)...\")\n",
        "    naive_mae_dict = {}\n",
        "\n",
        "    # Seasonality = 24 (Daily) for Hourly Data\n",
        "    SEASONALITY = 24\n",
        "\n",
        "    for series_id, group in df_train.groupby('series_id'):\n",
        "        y_hist = group['sales'].values\n",
        "        if len(y_hist) > SEASONALITY:\n",
        "            naive_mae = np.mean(np.abs(y_hist[SEASONALITY:] - y_hist[:-SEASONALITY]))\n",
        "        elif len(y_hist) > 1:\n",
        "            naive_mae = np.mean(np.abs(np.diff(y_hist)))\n",
        "        else:\n",
        "            naive_mae = 0.0\n",
        "        naive_mae_dict[series_id] = naive_mae\n",
        "\n",
        "    print(f\"   Computed Naive Errors for {len(naive_mae_dict)} series.\")\n",
        "\n",
        "    # 2. Setup Generators (The memory-safe part)\n",
        "    print(\"4. Setting up Data Generators...\")\n",
        "\n",
        "    # --- FIX: OVERLAPPING SPLIT LOGIC ---\n",
        "    # We determine the cutoff date for TRAINING targets.\n",
        "    # To generate validation samples starting immediately after train,\n",
        "    # the Val Generator needs LOOKBACK_WINDOW context from before the split.\n",
        "\n",
        "    # Cutoff is essentially the start of the validation period targets\n",
        "    max_train_date = df_train['date'].max()\n",
        "    # We reserve the last (VALIDATION_STEPS or HORIZON) hours for validation targets\n",
        "    cutoff_hours = max(VALIDATION_STEPS, FORECAST_HORIZON)\n",
        "    val_target_start_date = max_train_date - pd.Timedelta(hours=cutoff_hours)\n",
        "\n",
        "    # Train Mask: All data before the validation targets begin\n",
        "    # (Actually, we usually train on everything available up to split point)\n",
        "    mask_train = df_train['date'] < val_target_start_date\n",
        "\n",
        "    # Val Mask: Must include LOOKBACK_WINDOW before the targets begin to form the X input\n",
        "    val_input_start_date = val_target_start_date - pd.Timedelta(hours=LOOKBACK_WINDOW)\n",
        "    mask_val = df_train['date'] >= val_input_start_date\n",
        "\n",
        "    # We pass the dataframes directly to the generator\n",
        "    train_gen = TrafficGenerator(\n",
        "        df_train[mask_train].copy(),\n",
        "        LOOKBACK_WINDOW,\n",
        "        FORECAST_HORIZON,\n",
        "        stride=STRIDE\n",
        "    )\n",
        "\n",
        "    val_gen = TrafficGenerator(\n",
        "        df_train[mask_val].copy(),\n",
        "        LOOKBACK_WINDOW,\n",
        "        FORECAST_HORIZON,\n",
        "        stride=1,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # CLEAR RAM: We don't need the raw train dataframe anymore\n",
        "    del df_train\n",
        "    gc.collect()\n",
        "\n",
        "    # 3. Build Model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(LOOKBACK_WINDOW, 1)))\n",
        "    model.add(LSTM(units=30, activation='tanh'))\n",
        "    model.add(Dense(FORECAST_HORIZON))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    print(\"\\n--- Training LSTM Model ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=3,\n",
        "        validation_data=val_gen,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    if np.isnan(history.history['loss'][-1]):\n",
        "        print(\"!!! CRITICAL FAILURE: NaN loss detected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 4. Evaluation Loop (Batch of 5 to save RAM)\n",
        "    print(\"\\n--- Predicting & Calculating Metrics (Batch Size = 5) ---\")\n",
        "\n",
        "    metrics_list = []\n",
        "    BATCH_SIZE = 5\n",
        "    total_samples = len(ids_list)\n",
        "\n",
        "    for i in range(0, total_samples, BATCH_SIZE):\n",
        "        # Process in chunks\n",
        "        batch_ids = ids_list[i : i + BATCH_SIZE]\n",
        "        X_batch = X_predict[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Predict Batch\n",
        "        Y_log = model.predict(X_batch, verbose=0)\n",
        "        Y_log = np.nan_to_num(Y_log, nan=0.0)\n",
        "        Y_pred = np.expm1(Y_log)\n",
        "\n",
        "        # Evaluate Batch\n",
        "        for j, series_id in enumerate(batch_ids):\n",
        "            # Prediction for this series\n",
        "            y_p = Y_pred[j, :]\n",
        "\n",
        "            # Ground Truth\n",
        "            series_test_df = df_test[df_test['series_id'] == series_id].sort_values('date')\n",
        "            y_t = series_test_df['sales'].values[:FORECAST_HORIZON]\n",
        "\n",
        "            if len(y_t) < FORECAST_HORIZON:\n",
        "                continue\n",
        "\n",
        "            # Naive MAE (pre-calculated)\n",
        "            mae_n = naive_mae_dict.get(series_id, 0.0)\n",
        "\n",
        "            metrics = calculate_metrics(y_t, y_p, mae_n)\n",
        "            metrics['unique_id'] = series_id\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        # Clear Batch Memory\n",
        "        del X_batch, Y_log, Y_pred\n",
        "        gc.collect()\n",
        "\n",
        "        if (i // BATCH_SIZE) % 10 == 0:\n",
        "            print(f\"   Processed {min(i + BATCH_SIZE, total_samples)}/{total_samples} series...\", end='\\r')\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    total_seconds = time.time() - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*35)\n",
        "    print(\"FINAL TRAFFIC BENCHMARK RESULTS (LSTM - Hourly)\")\n",
        "    print(\"=\"*35)\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "    print(f\"Total Time: {total_seconds:.2f} seconds.\")\n",
        "    print(\"=\"*35)\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = load_and_prepare_traffic_data()\n",
        "\n",
        "    if data:\n",
        "        df_train, df_test, X_predict, ids_list = data\n",
        "        build_and_run_lstm(df_train, df_test, X_predict, ids_list)\n",
        "\n",
        "        print(\"\\n✅ LSTM Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "836856Rk-Q-7"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'ETTh1_Train.csv'\n",
        "TEST_FILE = 'ETTh1_Test.csv'\n",
        "\n",
        "# Standard Hourly Benchmark Settings\n",
        "LOOKBACK_WINDOW = 96   # 4 Days context\n",
        "FORECAST_HORIZON = 24  # 1 Day prediction\n",
        "VALIDATION_HOURS = 168 # 1 Week validation\n",
        "\n",
        "# ETTh1 is medium sized, so Stride 1 is fine (uses all data)\n",
        "STRIDE = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # ETTh1 can be negative (Temperature), so we don't clamp to 0 here.\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE (Standard version handles negative values via abs in denominator)\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE (Daily Seasonality = 24 hours)\n",
        "    SEASONALITY = 24\n",
        "    if len(y_train_hist) > SEASONALITY:\n",
        "        naive_errors = np.abs(y_train_hist[SEASONALITY:] - y_train_hist[:-SEASONALITY])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "\n",
        "# --- DATA PREPARATION ---\n",
        "\n",
        "def create_sequences_and_normalize(train_df, test_df, window, horizon, stride=1):\n",
        "    \"\"\"\n",
        "    Normalizes data (Z-Score) and creates sequences.\n",
        "    Returns: X_train, Y_train, X_predict, ids_list, Scaler_Dict\n",
        "    \"\"\"\n",
        "    X_train, Y_train = [], []\n",
        "    X_predict_list = []\n",
        "    ids_list = []\n",
        "\n",
        "    # Store means and stds to inverse transform predictions later\n",
        "    scaler_stats = {}\n",
        "\n",
        "    # Identify all series\n",
        "    series_ids = train_df['series_id'].unique()\n",
        "\n",
        "    print(f\"   Processing {len(series_ids)} series...\")\n",
        "\n",
        "    # Added tqdm for progress\n",
        "    for series_id in tqdm(series_ids, desc=\"   Creating Sequences\"):\n",
        "        # 1. Get Train Data\n",
        "        train_vals = train_df[train_df['series_id'] == series_id]['sales'].values.astype('float32')\n",
        "\n",
        "        # 2. Calculate Stats (Z-Score Normalization)\n",
        "        # We calculate stats ONLY on training data to avoid leakage\n",
        "        mean = np.mean(train_vals)\n",
        "        std = np.std(train_vals)\n",
        "        if std == 0: std = 1e-5 # Prevent div by zero\n",
        "\n",
        "        scaler_stats[series_id] = {'mean': mean, 'std': std}\n",
        "\n",
        "        # 3. Normalize Train Data\n",
        "        train_norm = (train_vals - mean) / std\n",
        "\n",
        "        # 4. Create Train Sequences\n",
        "        if len(train_norm) >= window + horizon:\n",
        "            for i in range(0, len(train_norm) - window - horizon + 1, stride):\n",
        "                X_train.append(train_norm[i:i + window])\n",
        "                Y_train.append(train_norm[i + window: i + window + horizon])\n",
        "\n",
        "        # 5. Prepare Prediction Input (Tail of Train) for Test\n",
        "        # We need the last 'window' points of training to predict the first 'horizon' points of test\n",
        "        if len(train_norm) >= window:\n",
        "            X_predict_list.append(train_norm[-window:])\n",
        "            ids_list.append(series_id)\n",
        "        else:\n",
        "            # Pad if short\n",
        "            padded = np.pad(train_norm, (window - len(train_norm), 0), 'constant')\n",
        "            X_predict_list.append(padded)\n",
        "            ids_list.append(series_id)\n",
        "\n",
        "    X_train = np.array(X_train, dtype='float32').reshape(-1, window, 1)\n",
        "    Y_train = np.array(Y_train, dtype='float32')\n",
        "\n",
        "    X_predict = np.array(X_predict_list, dtype='float32').reshape(-1, window, 1)\n",
        "\n",
        "    return X_train, Y_train, X_predict, ids_list, scaler_stats\n",
        "\n",
        "def load_and_prepare_etth1_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found.\")\n",
        "        return None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force string IDs\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Standardize column names\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Parse Dates\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "    # Sort\n",
        "    df_train = df_train.sort_values(['series_id', 'date'])\n",
        "    df_test = df_test.sort_values(['series_id', 'date'])\n",
        "\n",
        "    # --- CREATE SEQUENCES ---\n",
        "    print(f\"2. Creating Sequences (Lookback={LOOKBACK_WINDOW}, Horizon={FORECAST_HORIZON})...\")\n",
        "\n",
        "    # Validation Split (Internal)\n",
        "    # For ETTh1, we usually just take the last part of Train as Val for Early Stopping\n",
        "    max_train_date = df_train['date'].max()\n",
        "    val_start_date = max_train_date - pd.Timedelta(hours=VALIDATION_HOURS + FORECAST_HORIZON)\n",
        "\n",
        "    train_subset = df_train[df_train['date'] < val_start_date].copy()\n",
        "    val_subset = df_train[df_train['date'] >= val_start_date].copy()\n",
        "\n",
        "    # Generate Train Sequences & Stats\n",
        "    X_train, Y_train, _, _, scaler_stats = create_sequences_and_normalize(train_subset, None, LOOKBACK_WINDOW, FORECAST_HORIZON, stride=STRIDE)\n",
        "\n",
        "    # Generate Val Sequences (using same stats)\n",
        "    X_val, Y_val = [], []\n",
        "    for series_id in val_subset['series_id'].unique():\n",
        "        if series_id not in scaler_stats: continue\n",
        "\n",
        "        stats = scaler_stats[series_id]\n",
        "        vals = val_subset[val_subset['series_id'] == series_id]['sales'].values.astype('float32')\n",
        "        norm = (vals - stats['mean']) / stats['std']\n",
        "\n",
        "        if len(norm) >= LOOKBACK_WINDOW + FORECAST_HORIZON:\n",
        "            for i in range(len(norm) - LOOKBACK_WINDOW - FORECAST_HORIZON + 1):\n",
        "                X_val.append(norm[i:i+LOOKBACK_WINDOW])\n",
        "                Y_val.append(norm[i+LOOKBACK_WINDOW:i+LOOKBACK_WINDOW+FORECAST_HORIZON])\n",
        "\n",
        "    X_val = np.array(X_val, dtype='float32').reshape(-1, LOOKBACK_WINDOW, 1)\n",
        "    Y_val = np.array(Y_val, dtype='float32')\n",
        "\n",
        "    # Generate Prediction Input (X_predict) using Full Train and stats\n",
        "    print(\"3. Preparing Prediction Inputs...\")\n",
        "    # We re-run the create function on the FULL train set to get the X_predict for the Test set\n",
        "    # We discard the X_train/Y_train outputs here, we just want X_predict\n",
        "    _, _, X_predict, ids_list, _ = create_sequences_and_normalize(df_train, None, LOOKBACK_WINDOW, FORECAST_HORIZON, stride=STRIDE)\n",
        "\n",
        "    print(f\"   Train Shape: {X_train.shape}\")\n",
        "    print(f\"   Val Shape:   {X_val.shape}\")\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test\n",
        "\n",
        "\n",
        "def build_and_run_lstm(X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test):\n",
        "\n",
        "    # 1. Build Model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(LOOKBACK_WINDOW, 1)))\n",
        "    model.add(LSTM(units=64, activation='tanh')) # 64 units for Hourly complexity\n",
        "    model.add(Dense(FORECAST_HORIZON))\n",
        "\n",
        "    # SAFETY: Clipvalue to prevent explosion\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    print(\"\\n--- Training LSTM Model ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    val_data = (X_val, Y_val) if len(X_val) > 0 else None\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=5,\n",
        "        batch_size=128,\n",
        "        validation_data=val_data,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    if np.isnan(history.history['loss'][-1]):\n",
        "        print(\"!!! CRITICAL FAILURE: NaN loss detected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. Prediction\n",
        "    print(\"   Predicting...\")\n",
        "    # Prediction is in Z-Score scale\n",
        "    Y_pred_norm = model.predict(X_predict)\n",
        "\n",
        "    # 3. Evaluate Metrics (Inverse Transform per series)\n",
        "    print(\"\\n--- Calculating Metrics ---\")\n",
        "    metrics_list = []\n",
        "\n",
        "    # Create dictionary for Train History (for MASE)\n",
        "    history_dict = df_train.groupby('series_id')['sales'].apply(np.array).to_dict()\n",
        "\n",
        "    for i, series_id in enumerate(ids_list):\n",
        "        # Inverse Transform Prediction\n",
        "        stats = scaler_stats[series_id]\n",
        "        y_pred = (Y_pred_norm[i, :] * stats['std']) + stats['mean']\n",
        "\n",
        "        # Get Ground Truth (First 24 hours of Test)\n",
        "        series_test_df = df_test[df_test['series_id'] == series_id].sort_values('date')\n",
        "        y_true = series_test_df['sales'].values[:FORECAST_HORIZON]\n",
        "\n",
        "        if len(y_true) < FORECAST_HORIZON:\n",
        "            continue\n",
        "\n",
        "        y_hist = history_dict.get(series_id, np.array([]))\n",
        "\n",
        "        metrics = calculate_metrics(y_true, y_pred, y_hist)\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    total_seconds = time.time() - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*35)\n",
        "    print(\"FINAL ETTh1 RESULTS (LSTM)\")\n",
        "    print(\"=\"*35)\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "    print(f\"Total Time: {total_seconds:.2f} seconds.\")\n",
        "    print(\"=\"*35)\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = load_and_prepare_etth1_data()\n",
        "\n",
        "    if data:\n",
        "        X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test = data\n",
        "        build_and_run_lstm(X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test)\n",
        "\n",
        "        print(\"\\n✅ ETTh1 LSTM Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "yMZAetZY-Tsp"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'ETTh1_Train.csv'\n",
        "TEST_FILE = 'ETTh1_Test.csv'\n",
        "\n",
        "# Validation size: Last 7 days (168 hours) of training data\n",
        "VALIDATION_HOURS = 168\n",
        "\n",
        "# Lag Shift: 1 means we use previous hour's data to predict current hour.\n",
        "LAG_SHIFT = 1\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # ETTh1 values (Temperature/Load) can be negative if normalized.\n",
        "    # If working with raw data (un-normalized), you might want to clamp.\n",
        "    # Assuming raw data here based on processing script, but removing strict 0 clamp to be safe.\n",
        "    # y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE (Daily Seasonality = 24 hours)\n",
        "    SEASONALITY = 24\n",
        "    if len(y_train_hist) > SEASONALITY:\n",
        "        naive_errors = np.abs(y_train_hist[SEASONALITY:] - y_train_hist[:-SEASONALITY])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Forecast Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "def generate_features(df):\n",
        "    \"\"\"Generates time-series features for Hourly ETTh1 data.\"\"\"\n",
        "\n",
        "    # --- 1. TIME FEATURES ---\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['weekday'] = df['date'].dt.weekday\n",
        "    df['hour'] = df['date'].dt.hour # CRITICAL FOR HOURLY DATA\n",
        "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n",
        "\n",
        "    # --- 2. CATEGORICAL FEATURES ---\n",
        "    # Encode Series ID (HUFL, HULL, OT, etc.)\n",
        "    encoder = LabelEncoder()\n",
        "    df['series_id_encoded'] = encoder.fit_transform(df['series_id'].astype(str))\n",
        "\n",
        "    # --- 3. LAG AND ROLLING WINDOW FEATURES ---\n",
        "    # We define a helper to shift strictly within a Series ID\n",
        "    def grouped_shift(col, shift_n):\n",
        "        return df.groupby('series_id')[col].shift(shift_n)\n",
        "\n",
        "    # Lags (Hourly Logic)\n",
        "    df['lag_1']   = grouped_shift('sales', LAG_SHIFT)\n",
        "    df['lag_6']   = grouped_shift('sales', LAG_SHIFT + 5)   # 6 hours ago\n",
        "    df['lag_12']  = grouped_shift('sales', LAG_SHIFT + 11)  # 12 hours ago\n",
        "    df['lag_24']  = grouped_shift('sales', LAG_SHIFT + 23)  # 1 day ago\n",
        "    df['lag_168'] = grouped_shift('sales', LAG_SHIFT + 167) # 1 week ago\n",
        "\n",
        "    # Rolling Statistics\n",
        "    # Rolling Mean of last 24 hours (shifted to avoid leakage)\n",
        "    df['rolling_mean_24'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(24).mean()\n",
        "    )\n",
        "    df['rolling_std_24'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(24).std()\n",
        "    )\n",
        "\n",
        "    # Rolling Mean of last 1 week\n",
        "    df['rolling_mean_168'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(168).mean()\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_etth1_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found. Run the processing script first.\")\n",
        "        return None\n",
        "\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force Series ID to string\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Rename 'value' -> 'sales' for internal consistency with metrics\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Mark splits before merging\n",
        "    df_train['is_test'] = False\n",
        "    df_test['is_test'] = True\n",
        "\n",
        "    # Combine for Feature Engineering\n",
        "    full_df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "    full_df = full_df.sort_values(['series_id', 'date'])\n",
        "\n",
        "    print(\"2. Generating Features...\")\n",
        "    full_df = generate_features(full_df)\n",
        "\n",
        "    # --- SPLITTING ---\n",
        "    X_test_full = full_df[full_df['is_test'] == True].copy()\n",
        "    train_full = full_df[full_df['is_test'] == False].copy()\n",
        "\n",
        "    # Create Validation Set (Last 168 hours of Train)\n",
        "    max_train_date = train_full['date'].max()\n",
        "    val_start_date = max_train_date - pd.Timedelta(hours=VALIDATION_HOURS - 1)\n",
        "\n",
        "    X_train_df = train_full[train_full['date'] < val_start_date]\n",
        "    X_val_df = train_full[train_full['date'] >= val_start_date]\n",
        "\n",
        "    # Define Feature Columns\n",
        "    drop_cols = ['sales', 'date', 'series_id', 'is_test', 'split_type']\n",
        "    features = [c for c in full_df.columns if c not in drop_cols]\n",
        "\n",
        "    print(f\"   Features used: {features}\")\n",
        "\n",
        "    # Create Final Arrays\n",
        "    X_train = X_train_df[features]\n",
        "    y_train = X_train_df['sales']\n",
        "\n",
        "    X_val = X_val_df[features]\n",
        "    y_val = X_val_df['sales']\n",
        "\n",
        "    X_test = X_test_full[features]\n",
        "    y_test = X_test_full['sales']\n",
        "\n",
        "    # Clean NaNs caused by lags\n",
        "    valid_indices = X_train.dropna().index\n",
        "    X_train = X_train.loc[valid_indices]\n",
        "    y_train = y_train.loc[valid_indices]\n",
        "\n",
        "    print(\"\\n--- Final Data Shapes ---\")\n",
        "    print(f\"Train Set: {len(X_train)} rows\")\n",
        "    print(f\"Val Set:   {len(X_val)} rows\")\n",
        "    print(f\"Test Set:  {len(X_test)} rows\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full\n",
        "\n",
        "def train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full_history):\n",
        "\n",
        "    print(\"\\n--- Training XGBoost Model ---\")\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'rmse',\n",
        "        'eta': 0.05,\n",
        "        'max_depth': 8,\n",
        "        'seed': 42,\n",
        "        'nthread': -1,\n",
        "        'tree_method': 'hist'\n",
        "    }\n",
        "    num_round = 1000\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    eval_list = [(dtrain, 'train'), (dval, 'validation')]\n",
        "\n",
        "    bst = xgb.train(\n",
        "        xgb_params,\n",
        "        dtrain,\n",
        "        num_round,\n",
        "        evals=eval_list,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    print(f\"   Best Iteration: {bst.best_iteration}\")\n",
        "\n",
        "    # Prediction\n",
        "    y_pred_test = bst.predict(dtest, iteration_range=(0, bst.best_iteration))\n",
        "\n",
        "    # --- METRICS CALCULATION ---\n",
        "    print(\"\\n--- Calculating Final Benchmarks ---\")\n",
        "\n",
        "    results_df = X_test_full.copy()\n",
        "    results_df['y_pred'] = y_pred_test\n",
        "    results_df['sales'] = y_test\n",
        "\n",
        "    metrics_list = []\n",
        "    unique_series = results_df['series_id'].unique()\n",
        "\n",
        "    for i, series_id in enumerate(unique_series):\n",
        "        series_results = results_df[results_df['series_id'] == series_id]\n",
        "\n",
        "        history_series = train_full_history[train_full_history['series_id'] == series_id]['sales'].values\n",
        "\n",
        "        metrics = calculate_metrics(\n",
        "            series_results['sales'].values,\n",
        "            series_results['y_pred'].values,\n",
        "            history_series\n",
        "        )\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data_tuple = load_and_prepare_etth1_data()\n",
        "\n",
        "    if data_tuple:\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full = data_tuple\n",
        "        train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full)\n",
        "\n",
        "        print(\"\\n✅ ETTh1 XGBoost Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "U3qyrAPz-W4T"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'ETTh1_Train.csv'\n",
        "TEST_FILE = 'ETTh1_Test.csv'\n",
        "\n",
        "# Seasonality for Hourly Data (ETTh1)\n",
        "# Primary cycle is Daily (24 hours).\n",
        "# SNaive will repeat the last 24 hours of training into the future.\n",
        "SEASONALITY = 24\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # ETTh1 values can be negative (normalized), so we don't strict clamp to 0.\n",
        "    # y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # Calculate seasonal naive MAE on the entire training history\n",
        "    if len(y_train_hist) > SEASONALITY:\n",
        "        naive_errors = np.abs(y_train_hist[SEASONALITY:] - y_train_hist[:-SEASONALITY])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Forecast Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "\n",
        "def load_etth1_data():\n",
        "    \"\"\"Loads the pre-split ETTh1 CSVs.\"\"\"\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(f\"Error: {TRAIN_FILE} or {TEST_FILE} not found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force series_id to string\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Rename 'value' -> 'sales' (internal standard)\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Ensure dates are datetime objects\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "    print(f\"   Train Rows: {len(df_train)}\")\n",
        "    print(f\"   Test Rows:  {len(df_test)}\")\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def run_snaive_benchmark(df_train, df_test):\n",
        "    \"\"\"\n",
        "    Runs Seasonal Naive: Forecast = Last 24 hours of training, repeated.\n",
        "    \"\"\"\n",
        "    metrics_list = []\n",
        "\n",
        "    # Group by Series ID for fast access\n",
        "    train_groups = df_train.groupby('series_id')\n",
        "    test_groups = df_test.groupby('series_id')\n",
        "\n",
        "    unique_ids = df_test['series_id'].unique()\n",
        "\n",
        "    print(f\"\\n2. Running Seasonal Naive Benchmark (S={SEASONALITY}) on {len(unique_ids)} series...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, series_id in enumerate(unique_ids):\n",
        "        try:\n",
        "            # Get Training History (sorted by date)\n",
        "            train_series = train_groups.get_group(series_id).sort_values('date')\n",
        "            y_train = train_series['sales'].values\n",
        "\n",
        "            # Get Test Truth (sorted by date)\n",
        "            test_series = test_groups.get_group(series_id).sort_values('date')\n",
        "            y_test = test_series['sales'].values\n",
        "\n",
        "            # Prediction Horizon needed\n",
        "            horizon = len(y_test)\n",
        "\n",
        "            if len(y_train) < SEASONALITY:\n",
        "                # Fallback if history is too short (rare for ETTh1)\n",
        "                y_pred = np.tile(y_train[-1:], horizon)\n",
        "            else:\n",
        "                # SNaive Logic: Grab last 24 hours (1 Day)\n",
        "                last_season = y_train[-SEASONALITY:]\n",
        "\n",
        "                # Repeat (Tile) this pattern to fill the horizon\n",
        "                repetitions = int(np.ceil(horizon / SEASONALITY))\n",
        "                y_pred = np.tile(last_season, repetitions)[:horizon]\n",
        "\n",
        "            # Calculate Metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred, y_train)\n",
        "            metrics['unique_id'] = series_id\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Aggregate\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"\\n\\n--- Final SNaive Results ({total_time:.4f}s) ---\")\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_train, df_test = load_etth1_data()\n",
        "\n",
        "    if df_train is not None:\n",
        "        run_snaive_benchmark(df_train, df_test)\n",
        "        print(\"\\n✅ ETTh1 SNaive Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qK5kb7nT-ZiM"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Exchange_Rate_Train.csv'\n",
        "TEST_FILE = 'Exchange_Rate_Test.csv'\n",
        "\n",
        "# Exchange Rate is DAILY data.\n",
        "# While often modeled as a Random Walk (S=1), a standard SNaive benchmark\n",
        "# for daily data uses Weekly seasonality (S=7) to capture day-of-week effects.\n",
        "SEASONALITY = 7\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # Calculate seasonal naive MAE on the entire training history\n",
        "    if len(y_train_hist) > SEASONALITY:\n",
        "        naive_errors = np.abs(y_train_hist[SEASONALITY:] - y_train_hist[:-SEASONALITY])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Forecast Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "\n",
        "def load_exchange_data():\n",
        "    \"\"\"Loads the pre-split Exchange Rate CSVs.\"\"\"\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(f\"Error: {TRAIN_FILE} or {TEST_FILE} not found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force series_id to string\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Rename 'value' -> 'sales' (internal standard)\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Ensure dates are datetime objects\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "    print(f\"   Train Rows: {len(df_train)}\")\n",
        "    print(f\"   Test Rows:  {len(df_test)}\")\n",
        "\n",
        "    return df_train, df_test\n",
        "\n",
        "\n",
        "def run_snaive_benchmark(df_train, df_test):\n",
        "    \"\"\"\n",
        "    Runs Seasonal Naive: Forecast = Last 7 days of training, repeated.\n",
        "    \"\"\"\n",
        "    metrics_list = []\n",
        "\n",
        "    # Group by Series ID for fast access\n",
        "    train_groups = df_train.groupby('series_id')\n",
        "    test_groups = df_test.groupby('series_id')\n",
        "\n",
        "    unique_ids = df_test['series_id'].unique()\n",
        "\n",
        "    print(f\"\\n2. Running Seasonal Naive Benchmark (S={SEASONALITY}) on {len(unique_ids)} series...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i, series_id in enumerate(unique_ids):\n",
        "        try:\n",
        "            # Get Training History (sorted by date)\n",
        "            train_series = train_groups.get_group(series_id).sort_values('date')\n",
        "            y_train = train_series['sales'].values\n",
        "\n",
        "            # Get Test Truth (sorted by date)\n",
        "            test_series = test_groups.get_group(series_id).sort_values('date')\n",
        "            y_test = test_series['sales'].values\n",
        "\n",
        "            # Prediction Horizon needed\n",
        "            horizon = len(y_test)\n",
        "\n",
        "            if len(y_train) < SEASONALITY:\n",
        "                # Fallback if history is too short\n",
        "                y_pred = np.tile(y_train[-1:], horizon)\n",
        "            else:\n",
        "                # SNaive Logic: Grab last 7 days (Weekly Seasonality)\n",
        "                last_season = y_train[-SEASONALITY:]\n",
        "\n",
        "                # Repeat (Tile) this pattern to fill the horizon\n",
        "                repetitions = int(np.ceil(horizon / SEASONALITY))\n",
        "                y_pred = np.tile(last_season, repetitions)[:horizon]\n",
        "\n",
        "            # Calculate Metrics\n",
        "            metrics = calculate_metrics(y_test, y_pred, y_train)\n",
        "            metrics['unique_id'] = series_id\n",
        "            metrics_list.append(metrics)\n",
        "\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "\n",
        "    # Aggregate\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"\\n\\n--- Final SNaive Results ({total_time:.4f}s) ---\")\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_train, df_test = load_exchange_data()\n",
        "\n",
        "    if df_train is not None:\n",
        "        run_snaive_benchmark(df_train, df_test)\n",
        "        print(\"\\n✅ Exchange Rate SNaive Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7OasjHJh-bve"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Exchange_Rate_Train.csv'\n",
        "TEST_FILE = 'Exchange_Rate_Test.csv'\n",
        "\n",
        "# Forecast Horizon: 96 Days (Standard Long-Term Forecast for Exchange Rate)\n",
        "FORECAST_HORIZON = 96\n",
        "VALIDATION_DAYS = 96\n",
        "\n",
        "# Lag Shift: 1 means we use yesterday's data to predict today.\n",
        "LAG_SHIFT = 1\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # Exchange rates are strictly positive.\n",
        "    y_pred = np.maximum(1e-5, y_pred)\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # Exchange rate data typically doesn't have strong weekly seasonality like retail.\n",
        "    # It behaves more like a random walk. We use naive lag-1 for MASE denominator.\n",
        "    mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Forecast Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "def generate_features(df):\n",
        "    \"\"\"Generates time-series features for Daily Exchange Rate data.\"\"\"\n",
        "\n",
        "    # --- 1. TIME FEATURES ---\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['weekday'] = df['date'].dt.weekday\n",
        "    df['weekofyear'] = df['date'].dt.isocalendar().week.astype('int64')\n",
        "\n",
        "    # --- 2. CATEGORICAL FEATURES ---\n",
        "    # Encode Series ID (Country/Currency)\n",
        "    encoder = LabelEncoder()\n",
        "    df['series_id_encoded'] = encoder.fit_transform(df['series_id'].astype(str))\n",
        "\n",
        "    # --- 3. LAG AND ROLLING WINDOW FEATURES ---\n",
        "    # We define a helper to shift strictly within a Series ID\n",
        "    def grouped_shift(col, shift_n):\n",
        "        return df.groupby('series_id')[col].shift(shift_n)\n",
        "\n",
        "    # Lags (Daily Logic)\n",
        "    df['lag_1']  = grouped_shift('sales', LAG_SHIFT)\n",
        "    df['lag_7']  = grouped_shift('sales', LAG_SHIFT + 6)\n",
        "    df['lag_14'] = grouped_shift('sales', LAG_SHIFT + 13)\n",
        "    df['lag_30'] = grouped_shift('sales', LAG_SHIFT + 29)\n",
        "    df['lag_96'] = grouped_shift('sales', LAG_SHIFT + 95) # Horizon lag\n",
        "\n",
        "    # Rolling Statistics\n",
        "    # Rolling Mean of last 7/30 days (shifted to avoid leakage)\n",
        "    df['rolling_mean_7'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(7).mean()\n",
        "    )\n",
        "    df['rolling_std_7'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(7).std()\n",
        "    )\n",
        "\n",
        "    df['rolling_mean_30'] = df.groupby('series_id')['sales'].transform(\n",
        "        lambda x: x.shift(LAG_SHIFT).rolling(30).mean()\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_exchange_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found. Run the previous split script first.\")\n",
        "        return None\n",
        "\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force Series ID to string\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Rename 'value' -> 'sales' (internal standard naming)\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Mark splits before merging\n",
        "    df_train['is_test'] = False\n",
        "    df_test['is_test'] = True\n",
        "\n",
        "    # Combine for Feature Engineering (Preserve history at boundaries)\n",
        "    full_df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
        "    full_df = full_df.sort_values(['series_id', 'date'])\n",
        "\n",
        "    print(\"2. Generating Features...\")\n",
        "    full_df = generate_features(full_df)\n",
        "\n",
        "    # --- SPLITTING ---\n",
        "    # 1. Recover Test Set\n",
        "    X_test_full = full_df[full_df['is_test'] == True].copy()\n",
        "\n",
        "    # 2. Recover Train Set (All non-test data)\n",
        "    train_full = full_df[full_df['is_test'] == False].copy()\n",
        "\n",
        "    # 3. Create Validation Set from the END of the Train Set\n",
        "    max_train_date = train_full['date'].max()\n",
        "    val_start_date = max_train_date - pd.Timedelta(days=VALIDATION_DAYS - 1)\n",
        "\n",
        "    # Split Train/Val\n",
        "    X_train_df = train_full[train_full['date'] < val_start_date]\n",
        "    X_val_df = train_full[train_full['date'] >= val_start_date]\n",
        "\n",
        "    # Define Feature Columns (Drop non-feature cols)\n",
        "    drop_cols = ['sales', 'date', 'series_id', 'is_test', 'split_type']\n",
        "    features = [c for c in full_df.columns if c not in drop_cols]\n",
        "\n",
        "    print(f\"   Features used: {features}\")\n",
        "\n",
        "    # Create Final Arrays\n",
        "    X_train = X_train_df[features]\n",
        "    y_train = X_train_df['sales']\n",
        "\n",
        "    X_val = X_val_df[features]\n",
        "    y_val = X_val_df['sales']\n",
        "\n",
        "    X_test = X_test_full[features]\n",
        "    y_test = X_test_full['sales']\n",
        "\n",
        "    # Clean NaNs caused by lags (mostly in early train data)\n",
        "    valid_indices = X_train.dropna().index\n",
        "    X_train = X_train.loc[valid_indices]\n",
        "    y_train = y_train.loc[valid_indices]\n",
        "\n",
        "    print(\"\\n--- Final Data Shapes ---\")\n",
        "    print(f\"Train Set: {len(X_train)} rows\")\n",
        "    print(f\"Val Set:   {len(X_val)} rows\")\n",
        "    print(f\"Test Set:  {len(X_test)} rows\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full\n",
        "\n",
        "def train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full_history):\n",
        "\n",
        "    print(\"\\n--- Training XGBoost Model ---\")\n",
        "\n",
        "    xgb_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'eval_metric': 'rmse',\n",
        "        'eta': 0.05,\n",
        "        'max_depth': 6, # Slightly shallower for exchange rates (less noisy than retail)\n",
        "        'seed': 42,\n",
        "        'nthread': -1,\n",
        "        'tree_method': 'hist'\n",
        "    }\n",
        "    num_round = 1000\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "    dval = xgb.DMatrix(X_val, label=y_val)\n",
        "    dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "    eval_list = [(dtrain, 'train'), (dval, 'validation')]\n",
        "\n",
        "    bst = xgb.train(\n",
        "        xgb_params,\n",
        "        dtrain,\n",
        "        num_round,\n",
        "        evals=eval_list,\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=False\n",
        "    )\n",
        "\n",
        "    print(f\"   Best Iteration: {bst.best_iteration}\")\n",
        "\n",
        "    # Prediction\n",
        "    y_pred_test = bst.predict(dtest, iteration_range=(0, bst.best_iteration))\n",
        "\n",
        "    # --- METRICS CALCULATION ---\n",
        "    print(\"\\n--- Calculating Final Benchmarks ---\")\n",
        "\n",
        "    results_df = X_test_full.copy()\n",
        "    results_df['y_pred'] = y_pred_test\n",
        "    # Ensure alignment\n",
        "    results_df['sales'] = y_test\n",
        "\n",
        "    metrics_list = []\n",
        "\n",
        "    unique_series = results_df['series_id'].unique()\n",
        "\n",
        "    for i, series_id in enumerate(unique_series):\n",
        "        series_results = results_df[results_df['series_id'] == series_id]\n",
        "\n",
        "        # Use full training history (Train + Val) for MASE denominator\n",
        "        history_series = train_full_history[train_full_history['series_id'] == series_id]['sales'].values\n",
        "\n",
        "        metrics = calculate_metrics(\n",
        "            series_results['sales'].values,\n",
        "            series_results['y_pred'].values,\n",
        "            history_series\n",
        "        )\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Load data\n",
        "    data_tuple = load_and_prepare_exchange_data()\n",
        "\n",
        "    if data_tuple:\n",
        "        X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full = data_tuple\n",
        "\n",
        "        # Train and Eval\n",
        "        train_and_evaluate_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, X_test_full, train_full)\n",
        "\n",
        "        print(\"\\n✅ Exchange Rate XGBoost Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "w_8_8_E2-eL7"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Exchange_Rate_Train.csv'\n",
        "TEST_FILE = 'Exchange_Rate_Test.csv'\n",
        "\n",
        "# Standard Exchange Rate Benchmark Settings\n",
        "LOOKBACK_WINDOW = 96    # 96 Days context\n",
        "FORECAST_HORIZON = 96   # 96 Days prediction\n",
        "VALIDATION_DAYS = 96    # 96 Days validation\n",
        "\n",
        "# Exchange Rate data is small (~8 series, ~7k rows), so Stride 1 is fine\n",
        "STRIDE = 1\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# --- METRIC FUNCTIONS ---\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, y_train_hist):\n",
        "    \"\"\"Calculates MAE, MSE, RMSE, sMAPE, MASE, and Bias.\"\"\"\n",
        "\n",
        "    # Exchange rates are strictly positive\n",
        "    y_pred = np.maximum(0, y_pred)\n",
        "\n",
        "    # 1. MAE (Mean Absolute Error)\n",
        "    mae_forecast = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    # 2. MSE (Mean Squared Error)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "    # 3. RMSE\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # 4. sMAPE\n",
        "    denominator = np.abs(y_true) + np.abs(y_pred)\n",
        "    mask = denominator != 0\n",
        "    smape = np.mean(200 * np.abs(y_pred[mask] - y_true[mask]) / denominator[mask]) if np.sum(mask) > 0 else 0.0\n",
        "\n",
        "    # 5. MASE\n",
        "    # Exchange Rate is Daily. Often modeled as Random Walk (Naive 1).\n",
        "    # We use lag=1 for the denominator to be consistent with financial baselines.\n",
        "    # (If using SNaive with weekly seasonality, one might use 7, but 1 is standard for MASE here).\n",
        "    seasonality = 1\n",
        "    if len(y_train_hist) > seasonality:\n",
        "        naive_errors = np.abs(y_train_hist[seasonality:] - y_train_hist[:-seasonality])\n",
        "        mae_naive = np.mean(naive_errors)\n",
        "    elif len(y_train_hist) > 1:\n",
        "        mae_naive = np.mean(np.abs(np.diff(y_train_hist)))\n",
        "    else:\n",
        "        mae_naive = 0.0\n",
        "\n",
        "    mase = mae_forecast / mae_naive if mae_naive > 1e-9 else np.nan\n",
        "\n",
        "    # 6. Bias\n",
        "    sum_actual = np.sum(y_true)\n",
        "    bias = (np.sum(y_pred) - sum_actual) / sum_actual * 100 if sum_actual != 0 else np.nan\n",
        "\n",
        "    return {'MAE': mae_forecast, 'MSE': mse, 'RMSE': rmse, 'sMAPE': smape, 'MASE': mase, 'Bias': bias}\n",
        "\n",
        "\n",
        "# --- DATA PREPARATION ---\n",
        "\n",
        "def create_sequences_and_normalize(train_df, test_df, window, horizon, stride=1):\n",
        "    \"\"\"\n",
        "    Normalizes data (Z-Score) and creates sequences.\n",
        "    Returns: X_train, Y_train, X_predict, ids_list, Scaler_Dict\n",
        "    \"\"\"\n",
        "    X_train, Y_train = [], []\n",
        "    X_predict_list = []\n",
        "    ids_list = []\n",
        "\n",
        "    # Store means and stds to inverse transform predictions later\n",
        "    scaler_stats = {}\n",
        "\n",
        "    # Identify all series\n",
        "    series_ids = train_df['series_id'].unique()\n",
        "\n",
        "    print(f\"   Processing {len(series_ids)} series...\")\n",
        "\n",
        "    for series_id in tqdm(series_ids, desc=\"   Creating Sequences\"):\n",
        "        # 1. Get Train Data\n",
        "        train_vals = train_df[train_df['series_id'] == series_id]['sales'].values.astype('float32')\n",
        "\n",
        "        # 2. Calculate Stats (Z-Score Normalization)\n",
        "        # We calculate stats ONLY on training data to avoid leakage\n",
        "        mean = np.mean(train_vals)\n",
        "        std = np.std(train_vals)\n",
        "        if std == 0: std = 1e-5 # Prevent div by zero\n",
        "\n",
        "        scaler_stats[series_id] = {'mean': mean, 'std': std}\n",
        "\n",
        "        # 3. Normalize Train Data\n",
        "        train_norm = (train_vals - mean) / std\n",
        "\n",
        "        # 4. Create Train Sequences\n",
        "        if len(train_norm) >= window + horizon:\n",
        "            for i in range(0, len(train_norm) - window - horizon + 1, stride):\n",
        "                X_train.append(train_norm[i:i + window])\n",
        "                Y_train.append(train_norm[i + window: i + window + horizon])\n",
        "\n",
        "        # 5. Prepare Prediction Input (Tail of Train) for Test\n",
        "        # We need the last 'window' points of training to predict the first 'horizon' points of test\n",
        "        if len(train_norm) >= window:\n",
        "            X_predict_list.append(train_norm[-window:])\n",
        "            ids_list.append(series_id)\n",
        "        else:\n",
        "            # Pad if short\n",
        "            padded = np.pad(train_norm, (window - len(train_norm), 0), 'constant')\n",
        "            X_predict_list.append(padded)\n",
        "            ids_list.append(series_id)\n",
        "\n",
        "    X_train = np.array(X_train, dtype='float32').reshape(-1, window, 1)\n",
        "    Y_train = np.array(Y_train, dtype='float32')\n",
        "\n",
        "    X_predict = np.array(X_predict_list, dtype='float32').reshape(-1, window, 1)\n",
        "\n",
        "    return X_train, Y_train, X_predict, ids_list, scaler_stats\n",
        "\n",
        "def load_and_prepare_exchange_data():\n",
        "    print(\"1. Loading Data Splits...\")\n",
        "\n",
        "    if not os.path.exists(TRAIN_FILE) or not os.path.exists(TEST_FILE):\n",
        "        print(\"Error: Train/Test files not found.\")\n",
        "        return None\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(TRAIN_FILE, low_memory=False)\n",
        "    df_test = pd.read_csv(TEST_FILE, low_memory=False)\n",
        "\n",
        "    # Force string IDs\n",
        "    df_train['series_id'] = df_train['series_id'].astype(str)\n",
        "    df_test['series_id'] = df_test['series_id'].astype(str)\n",
        "\n",
        "    # Standardize column names\n",
        "    if 'value' in df_train.columns:\n",
        "        df_train.rename(columns={'value': 'sales'}, inplace=True)\n",
        "        df_test.rename(columns={'value': 'sales'}, inplace=True)\n",
        "\n",
        "    # Parse Dates\n",
        "    df_train['date'] = pd.to_datetime(df_train['date'])\n",
        "    df_test['date'] = pd.to_datetime(df_test['date'])\n",
        "\n",
        "    # Sort\n",
        "    df_train = df_train.sort_values(['series_id', 'date'])\n",
        "    df_test = df_test.sort_values(['series_id', 'date'])\n",
        "\n",
        "    # --- CREATE SEQUENCES ---\n",
        "    print(f\"2. Creating Sequences (Lookback={LOOKBACK_WINDOW}, Horizon={FORECAST_HORIZON})...\")\n",
        "\n",
        "    # Validation Split (Internal)\n",
        "    # Take the last part of Train as Val for Early Stopping\n",
        "    max_train_date = df_train['date'].max()\n",
        "    val_start_date = max_train_date - pd.Timedelta(days=VALIDATION_DAYS + FORECAST_HORIZON)\n",
        "\n",
        "    train_subset = df_train[df_train['date'] < val_start_date].copy()\n",
        "    val_subset = df_train[df_train['date'] >= val_start_date].copy()\n",
        "\n",
        "    # Generate Train Sequences & Stats\n",
        "    X_train, Y_train, _, _, scaler_stats = create_sequences_and_normalize(train_subset, None, LOOKBACK_WINDOW, FORECAST_HORIZON, stride=STRIDE)\n",
        "\n",
        "    # Generate Val Sequences (using same stats)\n",
        "    X_val, Y_val = [], []\n",
        "    for series_id in val_subset['series_id'].unique():\n",
        "        if series_id not in scaler_stats: continue\n",
        "\n",
        "        stats = scaler_stats[series_id]\n",
        "        vals = val_subset[val_subset['series_id'] == series_id]['sales'].values.astype('float32')\n",
        "        norm = (vals - stats['mean']) / stats['std']\n",
        "\n",
        "        if len(norm) >= LOOKBACK_WINDOW + FORECAST_HORIZON:\n",
        "            for i in range(len(norm) - LOOKBACK_WINDOW - FORECAST_HORIZON + 1):\n",
        "                X_val.append(norm[i:i+LOOKBACK_WINDOW])\n",
        "                Y_val.append(norm[i+LOOKBACK_WINDOW:i+LOOKBACK_WINDOW+FORECAST_HORIZON])\n",
        "\n",
        "    X_val = np.array(X_val, dtype='float32').reshape(-1, LOOKBACK_WINDOW, 1)\n",
        "    Y_val = np.array(Y_val, dtype='float32')\n",
        "\n",
        "    # Generate Prediction Input (X_predict) using Full Train and stats\n",
        "    print(\"3. Preparing Prediction Inputs...\")\n",
        "    _, _, X_predict, ids_list, _ = create_sequences_and_normalize(df_train, None, LOOKBACK_WINDOW, FORECAST_HORIZON, stride=STRIDE)\n",
        "\n",
        "    print(f\"   Train Shape: {X_train.shape}\")\n",
        "    print(f\"   Val Shape:   {X_val.shape}\")\n",
        "\n",
        "    return X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test\n",
        "\n",
        "\n",
        "def build_and_run_lstm(X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test):\n",
        "\n",
        "    # 1. Build Model\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(LOOKBACK_WINDOW, 1)))\n",
        "    model.add(LSTM(units=64, activation='tanh'))\n",
        "    model.add(Dense(FORECAST_HORIZON))\n",
        "\n",
        "    # SAFETY: Clipvalue to prevent explosion\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    print(\"\\n--- Training LSTM Model ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    val_data = (X_val, Y_val) if len(X_val) > 0 else None\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, Y_train,\n",
        "        epochs=10, # More epochs for smaller dataset\n",
        "        batch_size=64,\n",
        "        validation_data=val_data,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    if np.isnan(history.history['loss'][-1]):\n",
        "        print(\"!!! CRITICAL FAILURE: NaN loss detected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. Prediction\n",
        "    print(\"   Predicting...\")\n",
        "    # Prediction is in Z-Score scale\n",
        "    Y_pred_norm = model.predict(X_predict)\n",
        "\n",
        "    # 3. Evaluate Metrics (Inverse Transform per series)\n",
        "    print(\"\\n--- Calculating Metrics ---\")\n",
        "    metrics_list = []\n",
        "\n",
        "    # Create dictionary for Train History (for MASE)\n",
        "    history_dict = df_train.groupby('series_id')['sales'].apply(np.array).to_dict()\n",
        "\n",
        "    for i, series_id in enumerate(ids_list):\n",
        "        # Inverse Transform Prediction\n",
        "        stats = scaler_stats[series_id]\n",
        "        y_pred = (Y_pred_norm[i, :] * stats['std']) + stats['mean']\n",
        "\n",
        "        # Get Ground Truth (First 96 days of Test)\n",
        "        series_test_df = df_test[df_test['series_id'] == series_id].sort_values('date')\n",
        "        y_true = series_test_df['sales'].values[:FORECAST_HORIZON]\n",
        "\n",
        "        if len(y_true) < FORECAST_HORIZON:\n",
        "            continue\n",
        "\n",
        "        y_hist = history_dict.get(series_id, np.array([]))\n",
        "\n",
        "        metrics = calculate_metrics(y_true, y_pred, y_hist)\n",
        "        metrics['unique_id'] = series_id\n",
        "        metrics_list.append(metrics)\n",
        "\n",
        "    final_metrics = pd.DataFrame(metrics_list).dropna(subset=['MASE'])\n",
        "\n",
        "    total_seconds = time.time() - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\"*35)\n",
        "    print(\"FINAL EXCHANGE RATE RESULTS (LSTM)\")\n",
        "    print(\"=\"*35)\n",
        "    print(f\"Evaluated Series: {len(final_metrics)}\")\n",
        "    print(f\"Mean MAE:   {final_metrics['MAE'].mean():.4f}\")\n",
        "    print(f\"Mean MSE:   {final_metrics['MSE'].mean():.4f}\")\n",
        "    print(f\"Mean RMSE:  {final_metrics['RMSE'].mean():.4f}\")\n",
        "    print(f\"Mean sMAPE: {final_metrics['sMAPE'].mean():.4f}%\")\n",
        "    print(f\"Mean MASE:  {final_metrics['MASE'].mean():.4f}\")\n",
        "    print(f\"Mean Bias:  {final_metrics['Bias'].mean():.4f}%\")\n",
        "    print(f\"Total Time: {total_seconds:.2f} seconds.\")\n",
        "    print(\"=\"*35)\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    data = load_and_prepare_exchange_data()\n",
        "\n",
        "    if data:\n",
        "        X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test = data\n",
        "        build_and_run_lstm(X_train, Y_train, X_val, Y_val, X_predict, ids_list, scaler_stats, df_train, df_test)\n",
        "\n",
        "        print(\"\\n✅ Exchange Rate LSTM Benchmark Complete.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "jrfyCcbc-hsF"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TRAIN_FILE = 'Daily-train.csv'\n",
        "TEST_FILE = 'Daily-test.csv'\n",
        "INFO_FILE = 'M4-info.csv'\n",
        "\n",
        "def process_m4_data_fixed():\n",
        "    print(\"1. Loading Data...\")\n",
        "    df_train = pd.read_csv(TRAIN_FILE)\n",
        "    df_test = pd.read_csv(TEST_FILE)\n",
        "    df_info = pd.read_csv(INFO_FILE)\n",
        "\n",
        "    # Standardize ID columns\n",
        "    df_train.rename(columns={'V1': 'series_id'}, inplace=True)\n",
        "    df_test.rename(columns={'V1': 'series_id'}, inplace=True)\n",
        "    df_info.rename(columns={'M4id': 'series_id'}, inplace=True)\n",
        "\n",
        "    print(f\"   Train Shape: {df_train.shape}\")\n",
        "    print(f\"   Test Shape: {df_test.shape}\")\n",
        "\n",
        "    # --- STEP 2: ROBUST MELT (Skip complex metadata logic) ---\n",
        "    print(\"2. Melting Training Data...\")\n",
        "\n",
        "    # Identify value columns (V2, V3...)\n",
        "    train_value_cols = [c for c in df_train.columns if c.startswith('V')]\n",
        "\n",
        "    # Melt\n",
        "    df_train_long = df_train.melt(\n",
        "        id_vars=['series_id'],\n",
        "        value_vars=train_value_cols,\n",
        "        var_name='col_code',\n",
        "        value_name='sales'\n",
        "    )\n",
        "\n",
        "    # Drop NaNs (This removes the empty tail of the wide matrix)\n",
        "    df_train_long.dropna(subset=['sales'], inplace=True)\n",
        "\n",
        "    # Calculate Day Offset (V2 -> 0, V3 -> 1...)\n",
        "    # Extracts digits from 'V2' -> 2, subtracts 2 -> 0\n",
        "    df_train_long['day_offset'] = df_train_long['col_code'].str.extract('(\\d+)').astype(int) - 2\n",
        "\n",
        "    # Join with Start Date from Info file\n",
        "    print(\"   Mapping Dates...\")\n",
        "    df_train_final = df_train_long.merge(df_info[['series_id', 'StartingDate']], on='series_id', how='left')\n",
        "    df_train_final['start_date'] = pd.to_datetime(df_train_final['StartingDate'])\n",
        "\n",
        "    # Calculate Actual Date\n",
        "    df_train_final['date'] = df_train_final['start_date'] + pd.to_timedelta(df_train_final['day_offset'], unit='D')\n",
        "\n",
        "    # Final Cleanup\n",
        "    df_train_output = df_train_final[['series_id', 'date', 'sales']].sort_values(['series_id', 'date'])\n",
        "    df_train_output['split_type'] = 'TRAIN'\n",
        "\n",
        "    print(f\"   Saving M4_Daily_Train_PLX.csv ({len(df_train_output)} rows)...\")\n",
        "    df_train_output.to_csv('M4_Daily_Train_PLX.csv', index=False)\n",
        "\n",
        "\n",
        "    # --- STEP 3: PROCESS TEST DATA ---\n",
        "    print(\"3. Melting Test Data...\")\n",
        "\n",
        "    # Identify Test Value Columns\n",
        "    test_value_cols = [c for c in df_test.columns if c.startswith('V')]\n",
        "\n",
        "    df_test_long = df_test.melt(\n",
        "        id_vars=['series_id'],\n",
        "        value_vars=test_value_cols,\n",
        "        var_name='col_code',\n",
        "        value_name='sales'\n",
        "    )\n",
        "    df_test_long.dropna(subset=['sales'], inplace=True)\n",
        "\n",
        "    # Calculate Test Day Offset\n",
        "    # V2 in Test file is the 1st day of forecast.\n",
        "    # We need to find the END of the training data to know where Test starts.\n",
        "\n",
        "    # Get the max date per series from the training set we just built\n",
        "    train_end_dates = df_train_output.groupby('series_id')['date'].max().reset_index()\n",
        "    train_end_dates.rename(columns={'date': 'train_end_date'}, inplace=True)\n",
        "\n",
        "    # Merge this end date into the test set\n",
        "    df_test_final = df_test_long.merge(train_end_dates, on='series_id', how='left')\n",
        "\n",
        "    # Extract test offset (V2 -> 0, V3 -> 1...)\n",
        "    df_test_final['test_step'] = df_test_final['col_code'].str.extract('(\\d+)').astype(int) - 2\n",
        "\n",
        "    # Calculate Actual Test Date: Train End + 1 Day + Step Offset\n",
        "    df_test_final['date'] = df_test_final['train_end_date'] + pd.to_timedelta(df_test_final['test_step'] + 1, unit='D')\n",
        "\n",
        "    # Final Cleanup\n",
        "    df_test_output = df_test_final[['series_id', 'date', 'sales']].sort_values(['series_id', 'date'])\n",
        "    df_test_output['split_type'] = 'TEST'\n",
        "\n",
        "    print(f\"   Saving M4_Daily_Test_PLX.csv ({len(df_test_output)} rows)...\")\n",
        "    df_test_output.to_csv('M4_Daily_Test_PLX.csv', index=False)\n",
        "\n",
        "    print(\"\\n✅ SUCCESS! M4 Data Reprocessed correctly.\")\n",
        "    print(f\"Train Row Count: {len(df_train_output)}\")\n",
        "    print(f\"Test Row Count:  {len(df_test_output)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_m4_data_fixed()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qb2N3Qcm-lOC"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = 'traffic.csv'\n",
        "TRAIN_OUTPUT = 'Traffic_Daily_Train.csv'\n",
        "TEST_OUTPUT = 'Traffic_Daily_Test.csv'\n",
        "\n",
        "def process_traffic_data():\n",
        "    print(\"1. Loading Data...\")\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"Error: {INPUT_FILE} not found.\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    # Assuming standard traffic.csv where col 0 is date and rest are sensors\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "    # Ensure date is actually datetime objects\n",
        "    # Adjust 'date' below if your timestamp column has a different name\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "    else:\n",
        "        # Fallback if no header or different name, assuming first column is date\n",
        "        print(\"   Note: 'date' column not found, assuming first column is timestamp.\")\n",
        "        date_col_name = df.columns[0]\n",
        "        df.rename(columns={date_col_name: 'date'}, inplace=True)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    print(f\"   Total Data Shape: {df.shape}\")\n",
        "\n",
        "    # --- STEP 2: SPLIT DATA (TIME BASED) ---\n",
        "    print(\"2. Splitting Train/Test (Last 20% for Test)...\")\n",
        "\n",
        "    # Calculate split index based on time (rows)\n",
        "    total_rows = len(df)\n",
        "    split_index = int(total_rows * 0.8) # 80% cutoff\n",
        "\n",
        "    # Slice the dataframe chronologically\n",
        "    df_train_wide = df.iloc[:split_index].copy()\n",
        "    df_test_wide = df.iloc[split_index:].copy()\n",
        "\n",
        "    print(f\"   Train Rows (Wide): {len(df_train_wide)}\")\n",
        "    print(f\"   Test Rows (Wide):  {len(df_test_wide)}\")\n",
        "\n",
        "    # --- STEP 3: MELT AND PROCESS ---\n",
        "\n",
        "    def process_split(df_wide, split_name):\n",
        "        print(f\"   Processing {split_name} split...\")\n",
        "\n",
        "        # Identify sensor columns (all columns except 'date')\n",
        "        sensor_cols = [c for c in df_wide.columns if c != 'date']\n",
        "\n",
        "        # Melt from Wide (Date, Sensor1, Sensor2...) to Long (Date, Series_ID, Value)\n",
        "        df_long = df_wide.melt(\n",
        "            id_vars=['date'],\n",
        "            value_vars=sensor_cols,\n",
        "            var_name='series_id',\n",
        "            value_name='value'\n",
        "        )\n",
        "\n",
        "        # Sort for cleanliness\n",
        "        df_long = df_long.sort_values(['series_id', 'date'])\n",
        "\n",
        "        # Add metadata\n",
        "        df_long['split_type'] = split_name\n",
        "\n",
        "        return df_long\n",
        "\n",
        "    # Process Train\n",
        "    df_train_final = process_split(df_train_wide, 'TRAIN')\n",
        "\n",
        "    # Process Test\n",
        "    df_test_final = process_split(df_test_wide, 'TEST')\n",
        "\n",
        "    # --- STEP 4: SAVING ---\n",
        "    print(f\"3. Saving Output Files...\")\n",
        "\n",
        "    print(f\"   Saving {TRAIN_OUTPUT} ({len(df_train_final)} rows)...\")\n",
        "    df_train_final.to_csv(TRAIN_OUTPUT, index=False)\n",
        "\n",
        "    print(f\"   Saving {TEST_OUTPUT} ({len(df_test_final)} rows)...\")\n",
        "    df_test_final.to_csv(TEST_OUTPUT, index=False)\n",
        "\n",
        "    print(\"\\n✅ SUCCESS! Traffic Data Reprocessed correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_traffic_data()\n",
        "    \n",
        "    TRAIN_OUTPUT = 'Traffic_Daily_Train.csv'\n",
        "TEST_OUTPUT = 'Traffic_Daily_Test.csv'\n",
        "\n",
        "df_train = pd.read_csv(TRAIN_OUTPUT)\n",
        "df_test = pd.read_csv(TEST_OUTPUT)\n",
        "\n",
        "df_train.head()\n",
        "# df_test.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xsyPHIeb-ruS"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_SALES_FILE = 'sales_train_evaluation.csv'\n",
        "OUTPUT_SALES_FILE = 'sales_train_sample.csv' # Target file for upload\n",
        "M5_START_DATE = '2011-01-29'\n",
        "CHUNK_SIZE = 5000 # Read 5000 series (rows) at a time to find IDs\n",
        "SAMPLE_SIZE = 500 # The target sample size for the benchmark\n",
        "\n",
        "def transform_m5_sales_sample():\n",
        "    print(f\"Starting transformation and sampling of {INPUT_SALES_FILE}...\")\n",
        "    start_time = time.time()\n",
        "    total_output_rows = 0\n",
        "\n",
        "    # 1. --- PASS 1: IDENTIFY SAMPLE IDs (Low RAM usage) ---\n",
        "    print(\"PASS 1: Identifying 500 random series IDs (using random seed 42)...\")\n",
        "\n",
        "    try:\n",
        "        # Read only the 'id' column to get the complete list of IDs\n",
        "        id_df = pd.read_csv(INPUT_SALES_FILE, usecols=['id'])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: {INPUT_SALES_FILE} not found. Please ensure the file is accessible.\")\n",
        "        return\n",
        "\n",
        "    all_series_ids = id_df['id'].unique()\n",
        "\n",
        "    # Select the random sample (500 IDs)\n",
        "    if len(all_series_ids) > SAMPLE_SIZE:\n",
        "        np.random.seed(42) # Set seed for reproducible science\n",
        "        sampled_ids = np.random.choice(all_series_ids, size=SAMPLE_SIZE, replace=False)\n",
        "    else:\n",
        "        sampled_ids = all_series_ids\n",
        "\n",
        "    print(f\"Successfully selected {len(sampled_ids)} IDs.\")\n",
        "\n",
        "    # 2. --- PASS 2: MELT AND FILTER (Chunking/Low RAM usage) ---\n",
        "    print(\"PASS 2: Melting and filtering only sampled IDs...\")\n",
        "\n",
        "    # Clean output file\n",
        "    if os.path.exists(OUTPUT_SALES_FILE):\n",
        "        os.remove(OUTPUT_SALES_FILE)\n",
        "\n",
        "    is_first_chunk = True\n",
        "\n",
        "    # Read the full file in chunks (including all columns this time)\n",
        "    reader = pd.read_csv(INPUT_SALES_FILE, chunksize=CHUNK_SIZE)\n",
        "\n",
        "    for i, chunk in enumerate(reader):\n",
        "        # --- A. FILTER THE CHUNK ---\n",
        "        # Only keep rows whose 'id' is in our sampled list\n",
        "        chunk_filtered = chunk[chunk['id'].isin(sampled_ids)].copy()\n",
        "\n",
        "        if chunk_filtered.empty:\n",
        "            continue # Skip chunks that contain none of our target IDs\n",
        "\n",
        "        # --- B. MELT (Wide to Long Transformation) ---\n",
        "        day_cols = [c for c in chunk_filtered.columns if c.startswith('d_')]\n",
        "        id_cols = [c for c in chunk_filtered.columns if c not in day_cols]\n",
        "\n",
        "        m5_df_long = chunk_filtered.melt(\n",
        "            id_vars=id_cols,\n",
        "            value_vars=day_cols,\n",
        "            var_name='day_index',\n",
        "            value_name='sales_value'\n",
        "        ).dropna(subset=['sales_value'])\n",
        "\n",
        "        # --- C. GENERATE DATES ---\n",
        "        m5_df_long['day_num'] = m5_df_long['day_index'].str.replace('d_', '').astype(int)\n",
        "        m5_df_long['date'] = pd.to_datetime(M5_START_DATE) + pd.to_timedelta(m5_df_long['day_num'] - 1, unit='D')\n",
        "\n",
        "        # 3. Finalize Structure\n",
        "        final_df = m5_df_long.rename(columns={'id': 'series_id', 'sales_value': 'sales'})\n",
        "\n",
        "        # Select the required columns for the TimesFM benchmark and XGBoost features:\n",
        "        final_df = final_df[['series_id', 'date', 'sales', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']]\n",
        "\n",
        "        # 4. Write to CSV (Append mode after the first chunk)\n",
        "        header = is_first_chunk\n",
        "        mode = 'w' if is_first_chunk else 'a'\n",
        "\n",
        "        final_df.to_csv(OUTPUT_SALES_FILE, mode=mode, header=header, index=False)\n",
        "\n",
        "        total_output_rows += len(final_df)\n",
        "        is_first_chunk = False\n",
        "\n",
        "        print(f\"-> Chunk {i+1} processed. Total rows written: {total_output_rows}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"✅ M5 Sample Creation Complete\")\n",
        "    print(f\"Final File: {OUTPUT_SALES_FILE}\")\n",
        "    print(f\"Total Rows (Data Points): {total_output_rows}\")\n",
        "    print(f\"Total Time: {end_time - start_time:.2f} seconds\")\n",
        "    print(\"Ready for PLX upload.\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    transform_m5_sales_sample()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bS61nGgT-yE3"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = 'exchange_rate.csv'\n",
        "TRAIN_OUTPUT = 'Exchange_Rate_Train.csv'\n",
        "TEST_OUTPUT = 'Exchange_Rate_Test.csv'\n",
        "\n",
        "def process_exchange_rate_data():\n",
        "    print(\"1. Loading Data...\")\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"Error: {INPUT_FILE} not found.\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    # exchange_rate.csv often comes without headers (just 8 columns of rates) or with a date column.\n",
        "    # We attempt to load it standardly first.\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_FILE)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # --- DATE HANDLING ---\n",
        "    # Check if a date column exists. If not, we create a dummy time index.\n",
        "    date_col = None\n",
        "    for col in df.columns:\n",
        "        if 'date' in col.lower() or 'timestamp' in col.lower():\n",
        "            date_col = col\n",
        "            break\n",
        "\n",
        "    if date_col:\n",
        "        print(f\"   Detected date column: {date_col}\")\n",
        "        df['date'] = pd.to_datetime(df[date_col])\n",
        "        if date_col != 'date':\n",
        "            df.drop(columns=[date_col], inplace=True)\n",
        "    else:\n",
        "        print(\"   No date column found. Generating daily index starting 1990-01-01...\")\n",
        "        # Exchange Rate dataset (LSTNet version) is typically daily\n",
        "        df['date'] = pd.date_range(start='1990-01-01', periods=len(df), freq='D')\n",
        "\n",
        "    print(f\"   Total Data Shape: {df.shape}\")\n",
        "\n",
        "    # --- STEP 2: SPLIT DATA (TIME BASED) ---\n",
        "    print(\"2. Splitting Train/Test (Last 20% for Test)...\")\n",
        "\n",
        "    # Calculate split index based on rows (Time)\n",
        "    total_rows = len(df)\n",
        "    split_index = int(total_rows * 0.8) # 80% Train, 20% Test\n",
        "\n",
        "    # Slice chronologically\n",
        "    df_train_wide = df.iloc[:split_index].copy()\n",
        "    df_test_wide = df.iloc[split_index:].copy()\n",
        "\n",
        "    print(f\"   Train Rows (Wide): {len(df_train_wide)}\")\n",
        "    print(f\"   Test Rows (Wide):  {len(df_test_wide)}\")\n",
        "\n",
        "    # --- STEP 3: MELT AND PROCESS ---\n",
        "\n",
        "    def process_split(df_wide, split_name):\n",
        "        print(f\"   Processing {split_name} split...\")\n",
        "\n",
        "        # Identify series columns (all columns except 'date')\n",
        "        series_cols = [c for c in df_wide.columns if c != 'date']\n",
        "\n",
        "        # Melt from Wide (Date, Rate1, Rate2...) to Long (Date, Series_ID, Value)\n",
        "        df_long = df_wide.melt(\n",
        "            id_vars=['date'],\n",
        "            value_vars=series_cols,\n",
        "            var_name='series_id',\n",
        "            value_name='value'\n",
        "        )\n",
        "\n",
        "        # Sort for cleanliness\n",
        "        df_long = df_long.sort_values(['series_id', 'date'])\n",
        "\n",
        "        # Add metadata\n",
        "        df_long['split_type'] = split_name\n",
        "\n",
        "        return df_long\n",
        "\n",
        "    # Process Train\n",
        "    df_train_final = process_split(df_train_wide, 'TRAIN')\n",
        "\n",
        "    # Process Test\n",
        "    df_test_final = process_split(df_test_wide, 'TEST')\n",
        "\n",
        "    # --- STEP 4: SAVING ---\n",
        "    print(f\"3. Saving Output Files...\")\n",
        "\n",
        "    print(f\"   Saving {TRAIN_OUTPUT} ({len(df_train_final)} rows)...\")\n",
        "    df_train_final.to_csv(TRAIN_OUTPUT, index=False)\n",
        "\n",
        "    print(f\"   Saving {TEST_OUTPUT} ({len(df_test_final)} rows)...\")\n",
        "    df_test_final.to_csv(TEST_OUTPUT, index=False)\n",
        "\n",
        "    print(\"\\n✅ SUCCESS! Exchange Rate Data Reprocessed correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_exchange_rate_data()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qE98-Y3G-0Sb"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = 'ETTh1.csv'\n",
        "TRAIN_OUTPUT = 'ETTh1_Train.csv'\n",
        "TEST_OUTPUT = 'ETTh1_Test.csv'\n",
        "\n",
        "def process_etth1_data():\n",
        "    print(\"1. Loading Data...\")\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"Error: {INPUT_FILE} not found. Please upload ETTh1.csv\")\n",
        "        return\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "    # --- DATE HANDLING ---\n",
        "    # ETTh1 usually has a 'date' column.\n",
        "    if 'date' in df.columns:\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "    else:\n",
        "        print(\"   Warning: 'date' column not found. Looking for first column...\")\n",
        "        # Fallback: Assume first column is date\n",
        "        date_col = df.columns[0]\n",
        "        df.rename(columns={date_col: 'date'}, inplace=True)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    print(f\"   Total Data Shape: {df.shape}\")\n",
        "\n",
        "    # --- STEP 2: SPLIT DATA (TIME BASED) ---\n",
        "    print(\"2. Splitting Train/Test (Last 20% for Test)...\")\n",
        "\n",
        "    # Calculate split index based on rows (Time)\n",
        "    total_rows = len(df)\n",
        "    split_index = int(total_rows * 0.8) # 80% Train, 20% Test\n",
        "\n",
        "    # Slice chronologically\n",
        "    df_train_wide = df.iloc[:split_index].copy()\n",
        "    df_test_wide = df.iloc[split_index:].copy()\n",
        "\n",
        "    print(f\"   Train Rows (Wide): {len(df_train_wide)}\")\n",
        "    print(f\"   Test Rows (Wide):  {len(df_test_wide)}\")\n",
        "\n",
        "    # --- STEP 3: MELT AND PROCESS ---\n",
        "\n",
        "    def process_split(df_wide, split_name):\n",
        "        print(f\"   Processing {split_name} split...\")\n",
        "\n",
        "        # Identify feature columns (all columns except 'date')\n",
        "        # In ETTh1, these are typically: HUFL, HULL, MUFL, MULL, LUFL, LULL, OT\n",
        "        feature_cols = [c for c in df_wide.columns if c != 'date']\n",
        "\n",
        "        # Melt from Wide to Long\n",
        "        df_long = df_wide.melt(\n",
        "            id_vars=['date'],\n",
        "            value_vars=feature_cols,\n",
        "            var_name='series_id', # e.g., 'OT', 'HUFL'\n",
        "            value_name='value'\n",
        "        )\n",
        "\n",
        "        # Sort by Series then Date\n",
        "        df_long = df_long.sort_values(['series_id', 'date'])\n",
        "\n",
        "        # Add metadata\n",
        "        df_long['split_type'] = split_name\n",
        "\n",
        "        return df_long\n",
        "\n",
        "    # Process Train\n",
        "    df_train_final = process_split(df_train_wide, 'TRAIN')\n",
        "\n",
        "    # Process Test\n",
        "    df_test_final = process_split(df_test_wide, 'TEST')\n",
        "\n",
        "    # --- STEP 4: SAVING ---\n",
        "    print(f\"3. Saving Output Files...\")\n",
        "\n",
        "    print(f\"   Saving {TRAIN_OUTPUT} ({len(df_train_final)} rows)...\")\n",
        "    df_train_final.to_csv(TRAIN_OUTPUT, index=False)\n",
        "\n",
        "    print(f\"   Saving {TEST_OUTPUT} ({len(df_test_final)} rows)...\")\n",
        "    df_test_final.to_csv(TEST_OUTPUT, index=False)\n",
        "\n",
        "    print(\"\\n✅ SUCCESS! ETTh1 Data Reprocessed correctly.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_etth1_data()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
